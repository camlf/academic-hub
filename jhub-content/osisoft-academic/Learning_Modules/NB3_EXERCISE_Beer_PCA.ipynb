{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Using-Principal-Component-Analysis-(PCA)-on-Deschutes-Fermentation-Data\" data-toc-modified-id=\"Using-Principal-Component-Analysis-(PCA)-on-Deschutes-Fermentation-Data-1\">Using Principal Component Analysis (PCA) on Deschutes Fermentation Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Goal\" data-toc-modified-id=\"The-Goal-1.1\">The Goal</a></span><ul class=\"toc-item\"><li><span><a href=\"#Issues-to-consider\" data-toc-modified-id=\"Issues-to-consider-1.1.1\">Issues to consider</a></span></li></ul></li><li><span><a href=\"#This-Notebook\" data-toc-modified-id=\"This-Notebook-1.2\">This Notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1:-Define-System-Parameters\" data-toc-modified-id=\"Part-1:-Define-System-Parameters-1.2.1\">Part 1: Define System Parameters</a></span></li><li><span><a href=\"#Part-2:-Load-&amp;-Clean-Data\" data-toc-modified-id=\"Part-2:-Load-&amp;-Clean-Data-1.2.2\">Part 2: Load &amp; Clean Data</a></span></li><li><span><a href=\"#Part-3:-Define-Utility-Functions\" data-toc-modified-id=\"Part-3:-Define-Utility-Functions-1.2.3\">Part 3: Define Utility Functions</a></span></li><li><span><a href=\"#Part-4:-Process-and-Summarize-Data\" data-toc-modified-id=\"Part-4:-Process-and-Summarize-Data-1.2.4\">Part 4: Process and Summarize Data</a></span></li><li><span><a href=\"#Part-5:-Apply-PCA-to-FeatureDF\" data-toc-modified-id=\"Part-5:-Apply-PCA-to-FeatureDF-1.2.5\">Part 5: Apply PCA to <code>FeatureDF</code></a></span></li><li><span><a href=\"#Part-6-(BONUS):-Apply-clustering-to-Principal-Components:\" data-toc-modified-id=\"Part-6-(BONUS):-Apply-clustering-to-Principal-Components:-1.2.6\">Part 6 (BONUS): Apply clustering to Principal Components:</a></span></li></ul></li><li><span><a href=\"#Your-Task\" data-toc-modified-id=\"Your-Task-1.3\">Your Task</a></span></li><li><span><a href=\"#Part-1.-Define-System-Parameters\" data-toc-modified-id=\"Part-1.-Define-System-Parameters-1.4\">Part 1. Define System Parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#1a.-Define-relevant-main-variables\" data-toc-modified-id=\"1a.-Define-relevant-main-variables-1.4.1\">1a. Define relevant main variables</a></span></li><li><span><a href=\"#1b.-Define-auxiliary-variables\" data-toc-modified-id=\"1b.-Define-auxiliary-variables-1.4.2\">1b. Define auxiliary variables</a></span></li></ul></li><li><span><a href=\"#Part-2.-Load-&amp;-Clean-Data\" data-toc-modified-id=\"Part-2.-Load-&amp;-Clean-Data-1.5\">Part 2. Load &amp; Clean Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#2a.-Standard-OCS-initialization\" data-toc-modified-id=\"2a.-Standard-OCS-initialization-1.5.1\">2a. Standard OCS initialization</a></span></li><li><span><a href=\"#2b.-Download-data-from-OCS-with-Data-Views\" data-toc-modified-id=\"2b.-Download-data-from-OCS-with-Data-Views-1.5.2\">2b. Download data from OCS with Data Views</a></span></li><li><span><a href=\"#Find-predefined-Data-Views-for-Fermenter\" data-toc-modified-id=\"Find-predefined-Data-Views-for-Fermenter-1.5.3\">Find predefined Data Views for Fermenter</a></span></li><li><span><a href=\"#Data-View-Structure\" data-toc-modified-id=\"Data-View-Structure-1.5.4\">Data View Structure</a></span></li><li><span><a href=\"#Get-Interpolated-Data-from-Data-Views\" data-toc-modified-id=\"Get-Interpolated-Data-from-Data-Views-1.5.5\">Get Interpolated Data from Data Views</a></span></li><li><span><a href=\"#2c.-Clean-all_brands_df\" data-toc-modified-id=\"2c.-Clean-all_brands_df-1.5.6\">2c. Clean <code>all_brands_df</code></a></span></li></ul></li><li><span><a href=\"#Part-3.-Define-Utility-Functions\" data-toc-modified-id=\"Part-3.-Define-Utility-Functions-1.6\">Part 3. Define Utility Functions</a></span></li><li><span><a href=\"#Part-4.-Process-and-Summarize-Data\" data-toc-modified-id=\"Part-4.-Process-and-Summarize-Data-1.7\">Part 4. Process and Summarize Data</a></span></li><li><span><a href=\"#Part-5.-Apply-PCA-to-FeatureDF\" data-toc-modified-id=\"Part-5.-Apply-PCA-to-FeatureDF-1.8\">Part 5. Apply PCA to <code>FeatureDF</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#5a.-Define-and-scale-the-feature-space\" data-toc-modified-id=\"5a.-Define-and-scale-the-feature-space-1.8.1\">5a. Define and scale the feature space</a></span></li><li><span><a href=\"#5b.-Apply-PCA-to-the-feature-space\" data-toc-modified-id=\"5b.-Apply-PCA-to-the-feature-space-1.8.2\">5b. Apply PCA to the feature space</a></span></li><li><span><a href=\"#5c.-Find-number-of-principal-components-sufficent-to-explain-data\" data-toc-modified-id=\"5c.-Find-number-of-principal-components-sufficent-to-explain-data-1.8.3\">5c. Find number of principal components sufficent to explain data</a></span></li><li><span><a href=\"#5d.-Visualize-Deschutes-data-using-the-principal-components\" data-toc-modified-id=\"5d.-Visualize-Deschutes-data-using-the-principal-components-1.8.4\">5d. Visualize Deschutes data using the principal components</a></span></li><li><span><a href=\"#5e.-What's-in-an-outlier?\" data-toc-modified-id=\"5e.-What's-in-an-outlier?-1.8.5\">5e. What's in an outlier?</a></span></li></ul></li><li><span><a href=\"#Part-6.-Apply-clustering-to-Principal-Components-(BONUS)\" data-toc-modified-id=\"Part-6.-Apply-clustering-to-Principal-Components-(BONUS)-1.9\">Part 6. Apply clustering to Principal Components (BONUS)</a></span><ul class=\"toc-item\"><li><span><a href=\"#6a.-Apply-k-means-clustering-on-the-principal-components\" data-toc-modified-id=\"6a.-Apply-k-means-clustering-on-the-principal-components-1.9.1\">6a. Apply k-means clustering on the principal components</a></span></li><li><span><a href=\"#6b.-Plot-principal-components-and-color-by-kmeans-cluster\" data-toc-modified-id=\"6b.-Plot-principal-components-and-color-by-kmeans-cluster-1.9.2\">6b. Plot principal components and color by kmeans cluster</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using Principal Component Analysis (PCA) on Deschutes Fermentation Data\n",
    "---\n",
    "It is often useful to be able to quickly determine whether and which of the production batches are out-of-spec. The ability to determine the origin or the cause of a process failure or a bad batch is valuable to industry.\n",
    "\n",
    "In this notebook, we will use Principal Component Analysis (PCA) to determine which batches are out-of-spec without having to analyze the time series data for every attribute of erach beer batch, of which there are dozens. We will then use the properties of PCA to identify what may have caused a batch to be out-of-spec.\n",
    "\n",
    "First, make sure the latest version of OCS is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure latest version of OCS is installed\n",
    "!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ocs-academic-hub==0.56.0\n",
    "\n",
    "!pip install plotly_express==0.4.1 sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic Hub module \n",
    "from ocs_academic_hub import HubClient, timer\n",
    "\n",
    "# For OCS configuration and data manipulation\n",
    "import configparser\n",
    "from dateutil import parser\n",
    "import datetime as dt\n",
    "\n",
    "# analysis packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# packages for pca. sklearn is standard Python library for machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# packages for plotting\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Goal\n",
    "Use Principal Component Analysis (PCA) to identify the bad beer batches in 300 days worth of process data involving 5 beer brands (Realtime Hops, Alistair, Kerberos, Red Wonder, and Gray Horse), brewed across 6 fermentor vessels (31-36). \n",
    "\n",
    "Also, use the properties of Principal Components (PCs or \"scores\") to identify what features are most responsible for contributing to the out-of-spec qualities of outlier beer batches.\n",
    "\n",
    "### Issues to consider\n",
    "* Real process data is dirty (sensors can fail)\n",
    "* The stages may have been mislabeled (e.g. entries in \"Cooling\" phase may have been mislabled as being in \"Diacetyl Rest\")\n",
    "* Different cooling temperatures for each beer batch, and even for each zone in a fefermentor vessel for a given beer batch\n",
    "* There can be various volumes of beer in the fermentors\n",
    "* Data can have abrupt, aphysical jumps in temperature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## This Notebook\n",
    "\n",
    "### Part 1: Define System Parameters\n",
    "Specify parameters of interest like brands, fermentors, time period to analyze, time granularity, attributes.\n",
    "\n",
    "### Part 2: Load & Clean Data\n",
    "use OSIsoft Cloud Services (OCS) to obtain process data from Deschutes Brewery.\n",
    "\n",
    "### Part 3: Define Utility Functions\n",
    "Create utility functions for calculating the time elapsed per batch, identifying fermentation stages without relying on the \"Status\" labels, removing the bad batches, and summarizing the data.\n",
    "\n",
    "### Part 4: Process and Summarize Data\n",
    "Execute the functions created in Part 3 for each brand in each fermentor vessel. Collect the summary of the data into the dataframe `FeatureDF`.\n",
    "\n",
    "### Part 5: Apply PCA to `FeatureDF`\n",
    "`FeatureDF` is a high-dimensional dataset that is not easily human-processable. Use Principal Components Analysis (PCA) to obtain new components composed of linear combinations of the most \"relevant\" input features in `FeatureDF`( [5a](#section_5a)-[5c](#section_5c)); use those components to graphically visualize `FeatureDF` and identify the bad batches [5d](#section_5d); then identify the input features which contributed to the out-of-spec characteristics of the outlier batches ([5e](#section_5e)).\n",
    "\n",
    "### Part 6 (BONUS): Apply clustering to Principal Components:\n",
    "Use the k-means clustering algorithm to classify data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Your Task\n",
    "Some cells have missing pieces of code and are marked with `TODO` items in comments. Complete all the missing code to obtain a working notebook. \n",
    "\n",
    "If the notebook runs as inteded, using the default seetings in [Part 1a](#section_1a), one should obtain this plot in [Part 5e](#section_5e):\n",
    "\n",
    "![](https://academicpi.blob.core.windows.net/images/NB3_PCA_plot.png)\n",
    "\n",
    "(NOTE: `TODO` items can be found in [Part 3](#section_3) only, but this exercise will be easier if you go through it in sequential order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Define System Parameters\n",
    "---\n",
    "<a id=’section_1’></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Define relevant main variables\n",
    "<a id=’section_1a’></a>\n",
    "\n",
    "We want 300 days of brewing data in 6 fermentor vessels (31-36), beginning on March 17, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fermenters to analyze as a list of integers\n",
    "FERMENTORS_OF_INTEREST = [str(i) for i in range(31, 37)]\n",
    "\n",
    "# get brands of interest (leave as None to process all brands in df)\n",
    "BRANDS_OF_INTEREST = [\"Realtime Hops\", \"Red Wonder\", \"Alistair\", \"Kennedy\", \"Kerberos\"]\n",
    "\n",
    "# Use this option to study data for all available brands\n",
    "# BRANDS_OF_INTEREST = [\"Realtime Hops\", \"Red Wonder\", \"Alistair\", \"Kennedy\", \"Kerberos\", \"Grey Horse\", \"Adele\", \"Hop-Scotch\", \"5450\"]\n",
    "\n",
    "# relevant days over which to analyze fermentors\n",
    "TRAINING_DAYS = 300\n",
    "\n",
    "# Time period in which to analyze fermenters\n",
    "START_INDEX = \"2017-03-17T00:00:00\"\n",
    "END_INDEX = (parser.parse(START_INDEX) + dt.timedelta(days=TRAINING_DAYS)).isoformat()\n",
    "\n",
    "# define time grandularity for data. in this case, get events at 10 minute intervals\n",
    "INDEX_INTERVAL = \"00:10:00\"\n",
    "\n",
    "# EMERGENCIES ONLY! \n",
    "# If OCS not working, set to false \n",
    "USE_OCS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Define auxiliary variables\n",
    "<a id=’section_1b’></a>\n",
    "Make code more readable by avoiding to type long strings repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIC PV column names (temperatures)\n",
    "TIC_PV_COLUMNS = [\"Bottom TIC PV\", \"Middle TIC PV\", \"Top TIC PV\"]\n",
    "\n",
    "# TIC OUT column names (cooling intensities)\n",
    "TIC_OUT_COLUMNS = [\"Bottom TIC OUT\", \"Middle TIC OUT\", \"Top TIC OUT\"]\n",
    "\n",
    "# list of relevant attributes. these attributes should not have any bad inputs\n",
    "ATTRIBUTES_OF_INTEREST = (\n",
    "    [\"Brand\", \"Status\", \"Volume\", \"ADF\"] + TIC_PV_COLUMNS + TIC_OUT_COLUMNS\n",
    ")\n",
    "\n",
    "# list of possible values for bad inputs\n",
    "BAD_INPUTS = [\n",
    "    \"Comm Fail\",\n",
    "    \"I/O Timeout\",\n",
    "    \"Bad Input\",\n",
    "    \"none\",\n",
    "    \"\",\n",
    "    \"Calc Failed\",\n",
    "    \"Scan Off\",\n",
    "]\n",
    "\n",
    "# define stages involved in overall fermentation process\n",
    "ALL_BREWING_STAGES = [\n",
    "    \"Filling\",\n",
    "    \"Fermentation\",\n",
    "    \"Free Rise\",\n",
    "    \"Diacetyl Rest\",\n",
    "    \"Cooling\",\n",
    "    \"Maturation\",\n",
    "    \"Ready to Transfer\",\n",
    "    \"Emptying\",\n",
    "    \"Clean\",\n",
    "    \"CIP\",\n",
    "]\n",
    "\n",
    "# define stages which can actually affect beer quality\n",
    "IMPORTANT_BREWING_STAGES = ALL_BREWING_STAGES[\n",
    "    1:6\n",
    "]  # Fermentation->Free Rise->Diacetyl Rest->Cooling\n",
    "\n",
    "# list of the names of coordinates to be used for PCA. There are 32 total coords\n",
    "PClist = [\n",
    "    \"Brand\",  # brand identifier\n",
    "    \"Batch\",  # batch identifier\n",
    "    \"FERM_ADF\",  # variance of ADF during primary fermentation\n",
    "    \"COOL_ADF\",  # average adf after primary fermentation\n",
    "    \"Volume\",  # average volume after primary fermentation\n",
    "    \"FERM_Duration\",  # total time PRIMARY FERMENTATION lasted\n",
    "    \"FERM_avg_coolint_top\",  # average value for cooling intensity during PRIMARY FERMENTATION at different zones\n",
    "    \"FERM_avg_coolint_middle\",\n",
    "    \"FERM_avg_coolint_bottom\",\n",
    "    \"FERM_avg_temp_top\",  # average value for temperature during PRIMARY FERMENTATION at different zones\n",
    "    \"FERM_avg_temp_middle\",\n",
    "    \"FERM_avg_temp_bottom\",\n",
    "    \"FERM_max_temp_top\",  # maximum value for temperature during PRIMARY FERMENTATION at different zones\n",
    "    \"FERM_max_temp_middle\",\n",
    "    \"FERM_max_temp_bottom\",\n",
    "    \"FERM_min_temp_top\",  # minimum value for temperature during PRIMARY FERMENTATION at different zones\n",
    "    \"FERM_min_temp_middle\",\n",
    "    \"FERM_min_temp_bottom\",\n",
    "    \"FERM_var_temp_top\",  # variance of the temperature during PRIMARY FERMENTATION at different zones\n",
    "    \"FERM_var_temp_middle\",\n",
    "    \"FERM_var_temp_bottom\",\n",
    "    \"FERM_var_between_zones\",  # average of the variance between zones per event during PRIMARY FERMENTATION\n",
    "    \"COOL_Duration\",  # total time COOLING lasted\n",
    "    \"COOL_avg_coolint_top\",  # average value for cooling intensity during COOLING at different zones\n",
    "    \"COOL_avg_coolint_middle\",\n",
    "    \"COOL_avg_coolint_bottom\",\n",
    "    \"COOL_avg_temp_top\",  # average value for temperature during COOLING at different zones\n",
    "    \"COOL_avg_temp_middle\",\n",
    "    \"COOL_avg_temp_bottom\",\n",
    "    \"COOL_max_temp_top\",  # maximum value for temperature during COOLING at different zones\n",
    "    \"COOL_max_temp_middle\",\n",
    "    \"COOL_max_temp_bottom\",\n",
    "    \"COOL_min_temp_top\",  # minimum value for temperature during COOLING at different zones\n",
    "    \"COOL_min_temp_middle\",\n",
    "    \"COOL_min_temp_bottom\",\n",
    "    \"COOL_init_temp_top\",  # initial value for temperature during COOLING at different zones\n",
    "    \"COOL_init_temp_middle\",\n",
    "    \"COOL_init_temp_bottom\",\n",
    "    \"COOL_var_temp_top\",  # variance of the temperature during PRIMARY FERMENTATION at different zones\n",
    "    \"COOL_var_temp_middle\",\n",
    "    \"COOL_var_temp_bottom\",\n",
    "    \"COOL_var_between_zones\",  # average of the variance between zones per event during COOLING\n",
    "]\n",
    "\n",
    "# assign an integer value for each brand\n",
    "brand_dictionary = { brand:(i+1) for i, brand in enumerate(BRANDS_OF_INTEREST)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. Load & Clean Data\n",
    "---\n",
    "<a id=’section_2’></a>\n",
    "The Deschutes Brewery data needs to be retrieved from OSIsoft Cloud Services (OCS). \n",
    "\n",
    "We need to retrieve data for the `FERMENTORS_OF_INTEREST` from time period `START_INDEX` to `END_INDEX` at a resolution of `INDEX_INTERVAL` between each event. \n",
    "\n",
    "The data retrieved from OCS is loaded into a dataframe, which is then filtered to ensure it will not contain `BAD_INPUTS` for any value in the `ATTRIBUTES_OF_INTEREST` before it is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Standard OCS initialization\n",
    "<a id=’section_2a’></a>\n",
    "\n",
    "---\n",
    "**REQUIRED: a file `config.ini` should have been provided to you and put in the same directory as this notebook. Without this file executing the next cell will fail with `NoSectionError: No section: 'Access'` error message.**\n",
    "\n",
    "---\n",
    "\n",
    "Communication with OCS is done through a HubClient object. To get initialize one, we need to authenticate with the credentials in `config.ini`. The Deschutes dataset sits in a namespace called `fermenter_vessels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "hub_client = HubClient(\n",
    "    config.get(\"Access\", \"ApiVersion\"),\n",
    "    config.get(\"Access\", \"Tenant\"),\n",
    "    config.get(\"Access\", \"Resource\"),\n",
    "    config.get(\"Credentials\", \"ClientId\"),\n",
    "    config.get(\"Credentials\", \"ClientSecret\"),\n",
    ")\n",
    "\n",
    "namespace_id = config.get(\"Configurations\", \"Namespace\")\n",
    "print (f\"namespace_id: '{namespace_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Download data from OCS with Data Views\n",
    "<a id=’section_2b’></a>\n",
    "\n",
    "**Development tip (WARNING: executing dataview takes up to 30 secs)** \n",
    "\n",
    "Development of a notebook involves running code over and over, so you'll want to avoid long running steps when possible. This is why you can run the cell below once, with the resulting dataframe saved in variable `all_brands_df`. If you don't change any of its input parameters, `all_brands_df` is still valid and can be reused. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find predefined Data Views for Fermenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered to get the one related to \"PCA\"\n",
    "dv_ids = hub_client.fermenter_dataview_ids(\n",
    "    namespace_id,\n",
    "    \"PCA\",\n",
    "    first_fv=int(FERMENTORS_OF_INTEREST[0]),\n",
    "    last_fv=int(FERMENTORS_OF_INTEREST[-1]),\n",
    ")\n",
    "dv_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data View Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_client.dataview_definition(namespace_id, dv_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Interpolated Data from Data Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OCS:\n",
    "    # retrieve data from OCS\n",
    "    all_brands_df = hub_client.dataviews_interpolated_pd(\n",
    "        namespace_id, dv_ids, START_INDEX, END_INDEX, INDEX_INTERVAL\n",
    "    )\n",
    "else:\n",
    "    all_brands_df = pd.read_csv(\"PCA_BRANDS.csv\", sep=\",\")\n",
    "\n",
    "# preview `all_brands_df`\n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Clean `all_brands_df`\n",
    "<a id=’section_2c’></a>\n",
    "Drops all entries which have `NaN`s or `BAD_INPUTS` for all `ATTRIBUTES_OF_INTEREST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaNs in attributes of interest\n",
    "all_brands_df.dropna(subset=ATTRIBUTES_OF_INTEREST, inplace=True)\n",
    "\n",
    "# drop rows with bad inputs in attributes of interest\n",
    "for attribute in ATTRIBUTES_OF_INTEREST:\n",
    "    all_brands_df = all_brands_df[~all_brands_df[attribute].isin(BAD_INPUTS)]\n",
    "\n",
    "# ensure all entries for each attribute of interest has the right type\n",
    "for attribute in ATTRIBUTES_OF_INTEREST[2:]:\n",
    "    all_brands_df.loc[:, attribute] = pd.to_numeric(all_brands_df.loc[:, attribute])\n",
    "\n",
    "# recast Dataview_ID into Vessel_ID\n",
    "all_brands_df.rename(columns={\"Dataview_ID\": \"Vessel_ID\"}, inplace=True)\n",
    "\n",
    "\n",
    "all_brands_df[\"Vessel_ID\"] = all_brands_df[\"Vessel_ID\"].apply(\n",
    "    lambda vessel: vessel[-2:]\n",
    ")\n",
    "\n",
    "# all_brands_df.describe(include='all')\n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Define Utility Functions\n",
    "---\n",
    "<a id=’section_3’></a>\n",
    "Given a dataframe containing the fermentation data for one brand, first find all the different fermentation batches; then for each batch, we need to identify which stage each event belongs to.\n",
    "\n",
    "However, the \"Status\" labels are unreliable, so the stages have to be indetified through other means. Since it can be difficult to identify the transition between different stages, we consider only two rough categories of stages: (1) Precooling and (2) Cooling.\n",
    "\n",
    "The  general \"Precooling\" stage will contain the literal \"Fermentation\", \"Free Rise\", \"Diacetyl Rest\" stages, while the general \"Cooling\" stage will contain the literal \"Cooling\" and \"Maturation\" stages. \n",
    "\n",
    "We then want to create a summary of each batch, such that we can capture the general property of each batch. The summary for each batch will be collected in the dataframe `FeatureDF`.\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "|`fermentation_times`| Identify when fermentation begins; offset timestamps to begin on fermentation starts; add unique label to each fermentation batch|\n",
    "|`remove_batches_without_phase` | Certain stages are critical to flavor profile of beer; batches without these stages are just bad|\n",
    "|`organize_by_cooling`| finds when cooling starts and then relabels statuses as being either in 'Precooling' or 'Cooling' stages. Necessary because entries may have been mislabelled.|\n",
    "|`summarize_by_batch`| summarizes the time evolution of 10 attributes into 41 scalars. the summary is stored in a dataframe `ProcessedData`|\n",
    "\n",
    "The utility functions can all be found in the cell below (don't forget to execute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoCoolingStagesFound(Exception):\n",
    "    pass\n",
    "\n",
    "def fermentation_times(brand_df, fermentation_starts, shift=0):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Identify when fermentation begins and & \n",
    "        offsets timestamps to start when fermentation begins. \n",
    "        Also adds a label to each new fermentation batch\n",
    "    RETURNS: the input dataframe with offset timestamps, labels for each batch\n",
    "    \"\"\"\n",
    "\n",
    "    # create new df columns \"tsf\" and \"batch\". NOTE: \"tsf\" --> \"time since fermentation\"\n",
    "    # TODO: initialize \"tsf\" with 100000 and \"batch\" with -1\n",
    "    # =========== STUDENT BEGIN ==========\n",
    "  # brand_df = brand_df.assign(**{\"tsf\": 100000, @@@ Your code here @@@})\n",
    "    # =========== STUDENT END ==========\n",
    "\n",
    "    # reset index for fermentation starts, so that position index for .iterrows() is correct\n",
    "    fermentation_starts.reset_index(inplace=True)\n",
    "\n",
    "    # calculate the time elapsed for each fermentation step belonging to a new fermentation stage\n",
    "    for count, ferm_begin in fermentation_starts.iterrows():\n",
    "\n",
    "        # get the timestamp for each start of fermentation\n",
    "        fermentation_start_time = pd.Timestamp(ferm_begin[\"Timestamp\"])\n",
    "\n",
    "        # get booleans for new fermentation steps, where a new fermentation step is any step which exceeds the new fermentation time\n",
    "        new_fermentation_steps = (\n",
    "            brand_df[\"Timestamp\"].apply(lambda t: pd.Timestamp(t))\n",
    "            >= fermentation_start_time\n",
    "        )\n",
    "\n",
    "        # give each new fermentation step a new integer label\n",
    "        brand_df.loc[new_fermentation_steps, \"batch\"] = count + shift\n",
    "\n",
    "        # find time elapsed since start of fermentation, in units of float days\n",
    "        brand_df.loc[new_fermentation_steps, \"tsf\"] = brand_df.loc[\n",
    "            new_fermentation_steps, \"Timestamp\"\n",
    "        ].apply(\n",
    "            lambda t: (pd.Timestamp(t) - fermentation_start_time).total_seconds()\n",
    "            / 86400.0\n",
    "        )\n",
    "\n",
    "    # include only steps with non-negative \"batch\" labels and \"tsf\" not exceeding 100000 days\n",
    "    offset_df = brand_df[(brand_df[\"tsf\"] <= 100000) & (brand_df[\"batch\"] >= 0)]\n",
    "\n",
    "    # find the new label shift\n",
    "    shift = offset_df[\"batch\"].max()\n",
    "\n",
    "    return offset_df, shift\n",
    "\n",
    "\n",
    "def no_batch_without_phase(offset_df, ref_df):\n",
    "\n",
    "    # find all batches represented in `ref_df`; these are valid batches\n",
    "    valid_batches = ref_df[\"batch\"].unique()\n",
    "\n",
    "    # we wonly want batches in `valid_batches`\n",
    "    # TODO: Complete the filter expression\n",
    "    # =========== STUDENT BEGIN ==========\n",
    "  # offset_df = offset_df[@@@ Your code here @@@@.isin(@@@ Your code here @@@)]\n",
    "    # =========== STUDENT END ==========\n",
    "\n",
    "    return offset_df\n",
    "\n",
    "\n",
    "def organize_by_cooling(offset_df, fermentation_starts):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: finds when cooling starts and then relabels statuses as being either in\n",
    "        'Precooling' or 'Cooling' stages. Necessary because entries may have been mislabelled.\n",
    "    RETURNS: relabeled offset_df\n",
    "    \"\"\"\n",
    "\n",
    "    # find events which are stictly in cooling stage. When cooling intensity (TIC OUT) exceeds 99.99 in\n",
    "    # the three vessel zones, the batch is in the cooling stage.\n",
    "    # TODO: Complete the expression\n",
    "    # =========== STUDENT BEGIN ==========\n",
    "  # boolean = (\n",
    "  #     (@@@ Your code here @@@)\n",
    "  #     & (@@@ Your code here @@@)\n",
    "  #     & (@@@ Your code here @@@)\n",
    "  # )\n",
    "    # =========== STUDENT END ==========\n",
    "\n",
    "    # filter for events which are strictly for cooling\n",
    "    strict_cooling_df = offset_df.loc[boolean, :]\n",
    "\n",
    "    # the start of cooling is the first instance of a strict cooling event for a batch (given by 'batch')\n",
    "    cooling_starts = strict_cooling_df.groupby(\"batch\").first().reset_index()\n",
    "\n",
    "    # filter out batches with no cooling stages so it won't be used for PCA\n",
    "    offset_df = no_batch_without_phase(offset_df, cooling_starts)\n",
    "\n",
    "    # if there are no batches with cooling stages, don't continue with data processing\n",
    "    if offset_df.empty:\n",
    "        raise NoCoolingStagesFound\n",
    "        return None, None\n",
    "    else:\n",
    "        print(f\"\\t{len(cooling_starts)} Cooling starts\")\n",
    "\n",
    "    # relabel batch statuses as either 'pre-cooling' or 'cooling'\n",
    "    for __, row in cooling_starts.iterrows():\n",
    "\n",
    "        # label for current batch\n",
    "        current_batch = row[\"batch\"]\n",
    "\n",
    "        # when cooling starts for current batch\n",
    "        cooling_start_time = row[\"tsf\"]\n",
    "\n",
    "        # get booleans for current batch\n",
    "        boolean = offset_df[\"batch\"] == current_batch\n",
    "\n",
    "        # relabel Status as either \"Precooling\" or \"Cooling\"\n",
    "        # TODO: Complete the expression\n",
    "        # HINT: Status should be \"Precooling\" if time < cooling start time, \"Cooling\" otherwise\n",
    "        # =========== STUDENT BEGIN ==========\n",
    "      # offset_df.loc[boolean, \"Status\"] = offset_df.loc[boolean, \"tsf\"].apply(\n",
    "      #     lambda t: @@@ Your code here @@@ if  @@@ Your code here @@@ else \"Cooling\"\n",
    "      # )\n",
    "        # =========== STUDENT END ==========\n",
    "\n",
    "    return offset_df\n",
    "\n",
    "\n",
    "def summarize_by_batch(offset_df):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: summarizes the time evolution of 10 attributes into 41 scalars. \n",
    "    RETURNS: dataframe containing summary of processed data for one brand in a \n",
    "        fermentor vessel.\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique batch labels available in offset_df\n",
    "    all_batches = offset_df[\"batch\"].unique()\n",
    "\n",
    "    # get number of batches available in offset_df\n",
    "    num_batches = len(all_batches)\n",
    "\n",
    "    # initialize a pandas dataframe\n",
    "    PCA_df = pd.DataFrame(data=np.zeros((num_batches, len(PClist))), columns=PClist)\n",
    "\n",
    "    # fill PCA_df with summary of time series data per batch\n",
    "    for i, batch in enumerate(all_batches):\n",
    "\n",
    "        # divide offset_df for some given batch into cooling vs primary fermentation stages\n",
    "        ferm_df = offset_df[\n",
    "            (offset_df[\"batch\"] == batch) & (offset_df[\"Status\"] != \"Cooling\")\n",
    "        ]\n",
    "\n",
    "        # TODO: Complete the filter expression\n",
    "        # =========== STUDENT BEGIN ==========\n",
    "      # cool_df = offset_df[\n",
    "      #     (@@@ Your code here @@@) & (@@@ Your code here @@@)\n",
    "      # ]\n",
    "        # =========== STUDENT END ==========\n",
    "\n",
    "        # store batch, ADF, and column info in PCA_df\n",
    "        PCA_df.iloc[i, 0] = brand_dictionary[ferm_df[\"Brand\"].iloc[0]]\n",
    "        PCA_df.iloc[i, 1] = batch\n",
    "        PCA_df.iloc[i, 2] = ferm_df[\"ADF\"].mean()  # average ADF in precooling stage\n",
    "        PCA_df.iloc[i, 3] = cool_df[\"ADF\"].mean()  # average ADF in cooling stage\n",
    "        PCA_df.iloc[i, 4] = cool_df[\"Volume\"].mean()  # average Volume in cooling stage\n",
    "\n",
    "        # summaraize data in primary fermentation stages\n",
    "        PCA_df.iloc[i, 5] = ferm_df[\"tsf\"].max()  # duration of precooling stages\n",
    "        PCA_df.iloc[i, 6:12] = list(\n",
    "            ferm_df[TIC_OUT_COLUMNS + TIC_PV_COLUMNS].mean(axis=0)\n",
    "        )  # average values of temperature and cooling intensity for each zone during precooling\n",
    "        PCA_df.iloc[i, 12:15] = list(\n",
    "            ferm_df[TIC_PV_COLUMNS].max(axis=0)\n",
    "        )  # maximum temperature for each zone during precooling\n",
    "        PCA_df.iloc[i, 15:18] = list(\n",
    "            ferm_df[TIC_PV_COLUMNS].min(axis=0)\n",
    "        )  # minimum temperature for each zone during precooling\n",
    "        PCA_df.iloc[i, 18:21] = list(\n",
    "            ferm_df[TIC_PV_COLUMNS].var(axis=0)\n",
    "        )  # variance of the temperature of each zone cureing precooling\n",
    "        PCA_df.iloc[i, 21] = (\n",
    "            ferm_df[TIC_PV_COLUMNS].var(axis=1)\n",
    "        ).mean()  # average of the variance of the temperatures in each zone during precooling\n",
    "\n",
    "        # summarize data in cooling stages\n",
    "        PCA_df.iloc[i, 22] = (\n",
    "            cool_df[\"tsf\"].max() - cool_df[\"tsf\"].min()\n",
    "        )  # duration of cooling stages\n",
    "        PCA_df.iloc[i, 23:29] = list(\n",
    "            cool_df[TIC_OUT_COLUMNS + TIC_PV_COLUMNS].mean(axis=0)\n",
    "        )  # average values of temperature and cooling intensity for each zone during cooling\n",
    "        PCA_df.iloc[i, 29:32] = list(\n",
    "            cool_df[TIC_PV_COLUMNS].max(axis=0)\n",
    "        )  # maximum temperature for each zone during cooling\n",
    "        PCA_df.iloc[i, 32:35] = list(\n",
    "            cool_df[TIC_PV_COLUMNS].min(axis=0)\n",
    "        )  # minimum temperature for each zone during cooling\n",
    "        PCA_df.iloc[i, 35:38] = list(\n",
    "            cool_df[TIC_PV_COLUMNS].var(axis=0)\n",
    "        )  # variance of the temperature of each zone during cooling\n",
    "        PCA_df.iloc[i, 38] = (\n",
    "            cool_df[TIC_PV_COLUMNS].var(axis=1)\n",
    "        ).mean()  # average of the variance of the temperatures in each zone during cooling\n",
    "        PCA_df.iloc[i, 39:42] = list(\n",
    "            cool_df[TIC_PV_COLUMNS].iloc[0]\n",
    "        )  # initial temperature in cooling stage\n",
    "\n",
    "    return PCA_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. Process and Summarize Data \n",
    "---\n",
    "<a id=’section_4’></a>\n",
    "\n",
    "Run the cell below to call the previously defined utility functions and  create the dataframe `FeatureDF` which contains a summary of the time series data from `all_brands_df`. \n",
    "\n",
    "The columns of `FeatureDF` can be used as input features for Principal Component Analysis (PCA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe which will be later saved into .csv format to be analyzed by PCA\n",
    "FeatureDF = pd.DataFrame(columns=PClist)\n",
    "\n",
    "# initialize batch labels\n",
    "label_shift = 1\n",
    "\n",
    "generator = ((f, b) for f in FERMENTORS_OF_INTEREST for b in BRANDS_OF_INTEREST)\n",
    "for fermentor, brand in generator:\n",
    "    print(f\"\\n===> FV {fermentor}. Brand: {brand}\")\n",
    "\n",
    "    # create reduced df containing only data from relevant vessel and brand\n",
    "    brand_df = all_brands_df[\n",
    "        (all_brands_df[\"Vessel_ID\"] == fermentor) \n",
    "        & (all_brands_df[\"Brand\"] == brand)\n",
    "    ]\n",
    "    \n",
    "    # identify fermentation starts\n",
    "    fermentation_starts = brand_df[\n",
    "        (brand_df[\"Status\"] == \"Fermentation\")\n",
    "        & (brand_df[\"Status\"].shift(1) != \"Fermentation\")\n",
    "    ]\n",
    "    \n",
    "    # dont' consider data not in important brewing stages\n",
    "    brand_df = brand_df[brand_df[\"Status\"].isin(IMPORTANT_BREWING_STAGES)]\n",
    "    \n",
    "    # if fermentation did not occur, there is nothing to analyze\n",
    "    N_fermentations = len(fermentation_starts)\n",
    "    if N_fermentations == 0:\n",
    "        print (\"\\tNo Fermentation data to analyze!\")\n",
    "        continue\n",
    "    else:\n",
    "        print (f\"\\t{N_fermentations} Fermentation starts.\")\n",
    "        \n",
    "    # offset timestamps for brand_df, so t=0 when fermentation starts. \n",
    "    offset_df, shift = fermentation_times(brand_df, fermentation_starts, shift=label_shift)\n",
    "    \n",
    "    # also assign new labels for each brew batch\n",
    "    label_shift = shift+1\n",
    "    \n",
    "    # remove batches with no maturation phases\n",
    "    offset_df = no_batch_without_phase(offset_df, offset_df[offset_df[\"Status\"] == \"Maturation\"])\n",
    "    \n",
    "    # identify cooling starts; relabel relabel all Statuses before \"Cooling\" as \"Pre-cooling\"\n",
    "    # if cooling stages do not exist, pca not needed to determine whether batch is bad\n",
    "    try:\n",
    "        offset_df = organize_by_cooling(offset_df, fermentation_starts)\n",
    "    except NoCoolingStagesFound:\n",
    "        print (\"\\tNo Cooling data to analyze!\")\n",
    "        continue\n",
    "        \n",
    "    # summarize time series data into compact dataframe\n",
    "    FeatureDF = FeatureDF.append(summarize_by_batch(offset_df))\n",
    "    \n",
    "# clean indices\n",
    "FeatureDF = FeatureDF.reset_index()\n",
    "FeatureDF = FeatureDF.drop(columns=\"index\")\n",
    "        \n",
    "# write `FeatureDF` to .csv data file\n",
    "FeatureDF.to_csv(\"FeatureDF.csv\", sep=\",\", index=False)\n",
    "\n",
    "# preview processed data\n",
    "print(\"\\n\\nFinished data processing. `FeatureDF` looks like:\")\n",
    "FeatureDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5. Apply PCA to `FeatureDF`\n",
    "---\n",
    "<a id=’section_5’></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Define and scale the feature space\n",
    "<a ia=’section_5a’></a>\n",
    "The feature space `X` contains all the input features (i.e. columns) in `FeatureDF` which can potentially affect the flavor profile of the beers. \n",
    "\n",
    "When PCA is applied on `X`, we obtain new set of coordinates (\"principal components\") which are orthogonal to each other and point in the directions of maximum variance (eigenvectors).\n",
    "\n",
    "To prevent any one feature dominating the variance calculations, it is recommended that the feature space `X` be scaled. \n",
    "\n",
    "In this case, the feature space `X` is scaled by \"centering\" each feature and normalizing them by their standard deviation. Using sklearn, this can be accomplished using `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature space (variables which affect beer quality)\n",
    "X = FeatureDF.iloc[:, 2:]\n",
    "\n",
    "# M = number of observations (rows in X),  N = number of components in the feature space (columns in X)\n",
    "[M, N] = np.shape(X)\n",
    "\n",
    "# save unscaled feature space (will only be use for plotting purposes)\n",
    "Xold = X\n",
    "\n",
    "# scale the feature space\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>>>>>===================THIS NEXT IMMEDIATE CELL CAN BE IGNORED===================**\n",
    "\n",
    "Run the cell below to graphically compare the difference between scaled and unscaled features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def create_box_plot_object(feature_matrix, N_features, title):\n",
    "    plot_objects = []\n",
    "    for i in range(N_features):\n",
    "        plot_obj = go.Box(y=np.array(feature_matrix)[:, i], name=str(i + 1))\n",
    "        plot_objects.append(plot_obj)\n",
    "\n",
    "    fig = go.Figure(data=plot_objects, layout=go.Layout(title=title))\n",
    "    fig.show()\n",
    "    return\n",
    "\n",
    "# create a plotly object for the unscaled feature space\n",
    "create_box_plot_object(Xold, N, \"Unscaled Features\")\n",
    "\n",
    "# create a plotly object for the scaled feature space\n",
    "create_box_plot_object(X, N, \"Scaled Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===========================END IGNORABLE CELL===========================<<<<<**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Apply PCA to the feature space\n",
    "<a id=’section_5b’></a>\n",
    "\n",
    "When PCA is applied to the old feature space `X`, we can obtain a covariance (or correlation) matrix and subsequently eigenvectors and eigenvalues.\n",
    "\n",
    "The eigenvectors, or \"Principal Axes\", point in the directions of the new feature space, `T`, and in the directions of maximum variance of the old feature space `X`.\n",
    "\n",
    "The eigenvalues,`Lambda`, correspond the the magnitude of the Principal Axes. The eigenvalues reflect the amount of variance explained by the Principal Axes.\n",
    "\n",
    "The new feature space `T` is a matrix of \"scores\" or \"Principal Components\". Each Principal Component is a linear combination of the input features in `X`.\n",
    "\n",
    "The coefficients for these linear combinations are the elements of their corresponding Principal Axes or eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain maximum number of principal components\n",
    "n_comps = min([M, N])\n",
    "\n",
    "# initialize PCA and get covariance matrix\n",
    "covariance_matrix = PCA(n_components=n_comps)\n",
    "covariance_matrix.fit(X)\n",
    "\n",
    "# get matrix of principal components (a.k.a PCs or scores)\n",
    "T = covariance_matrix.transform(X)\n",
    "\n",
    "# get eigenvectors\n",
    "P = covariance_matrix.components_\n",
    "\n",
    "# get eigenvalues\n",
    "Lambda = covariance_matrix.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>>>>>===================THIS NEXT IMMEDIATE CELL CAN BE IGNORED===================**\n",
    "\n",
    "Visualize the first $n$ components and how they are related to the input features (columns in `FeatureDF`). A principal component is a linear combination of the input features, and the coefficients are the elements of its eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appropriate number of princiapl components to use\n",
    "first_n_components = 3\n",
    "\n",
    "# initialize plots\n",
    "fig = make_subplots(\n",
    "    rows=first_n_components, cols=1, shared_xaxes=True, vertical_spacing=0.03\n",
    ")\n",
    "\n",
    "for i_component in range(first_n_components):\n",
    "    plot_object = go.Bar(\n",
    "        x=FeatureDF.columns.tolist()[2:],\n",
    "        y=P[i_component, :],\n",
    "        name=f\"PC{i_component+1}\",\n",
    "    )\n",
    "\n",
    "    fig.append_trace(plot_object, i_component + 1, 1)\n",
    "    fig[\"layout\"][f\"yaxis{i_component+1}\"].update(title=\"Coefficients\")\n",
    "fig[\"layout\"].update(width=800, height=800, xaxis=dict(tickangle=45))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===========================END IGNORABLE CELL===========================<<<<<**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Find number of principal components sufficent to explain data\n",
    "<a id=’section_5c’></a>\n",
    "The eigenvalues of the covariance matrix is equivalent to the variance of a component. We want $l$ components which explain most of the variance in the data. A simple and widely used criterion is the cumulative percent variance (CPV).\n",
    "\n",
    "$ \n",
    "CPV(l) = 100 \\left( \n",
    "\\frac{\\sum_{j=1}^l \\lambda_j}\n",
    "{\\sum_{j=1}^M \\lambda_j} \n",
    "\\right) \\%\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the variance explained by each component, normalize it, and take the cumulative sum\n",
    "cumulative_variance = covariance_matrix.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# plotly object for cumulative variance data\n",
    "data = go.Scatter(\n",
    "    x=[item + 1 for item in range(len(cumulative_variance))],\n",
    "    y=[100.0 * item for item in list(cumulative_variance)],\n",
    "    mode=\"lines+markers\",\n",
    ")\n",
    "\n",
    "# plotly object for reference CPV\n",
    "lines = []\n",
    "for var in [75, 87, 99]:\n",
    "    lines.append(\n",
    "        go.Scatter(\n",
    "            x=[0] + [item + 1 for item in range(len(cumulative_variance))],\n",
    "            y=[var] * (len(cumulative_variance) + 1),\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\", dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# plot layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Number of Principal Components\"),\n",
    "    yaxis=dict(title=\"Cumulative Variance (%)\"),\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# create and show plot\n",
    "fig = go.Figure(data=[data] + lines, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many PCs do you think you should use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Visualize Deschutes data using the principal components\n",
    "<a id='section_5d'></a>\n",
    "Regardless of the appropriate number of principal components from CPV, we are limited to visualizing at most three principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list for plot objects\n",
    "obj_list = []\n",
    "\n",
    "# create plot objects for each brand\n",
    "for i_brand, brand in enumerate(BRANDS_OF_INTEREST):\n",
    "\n",
    "    # get which entries in ProcessedData correspond to rband\n",
    "    boolean = FeatureDF[\"Brand\"] == brand_dictionary[brand]\n",
    "\n",
    "    # create plotly object\n",
    "    plot_obj = go.Scatter3d(\n",
    "        x=T[boolean, 0],\n",
    "        y=T[boolean, 1],\n",
    "        z=T[boolean, 2],\n",
    "        text=list(FeatureDF.loc[boolean, \"Batch\"]),\n",
    "        hoverinfo=\"text\",\n",
    "        name=brand,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=7, opacity=0.75),\n",
    "    )\n",
    "\n",
    "    # append to list of plotly objects\n",
    "    obj_list.append(plot_obj)\n",
    "\n",
    "# create layout\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"PC 1\"), yaxis=dict(title=\"PC 2\"), zaxis=dict(title=\"PC 3\")\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    ")\n",
    "\n",
    "# plot!\n",
    "fig = go.Figure(data=obj_list, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5e. What's in an outlier?\n",
    "<a id=’section_5e’></a>\n",
    "We can use **contribution plots** to identify which input features are responsible for making an outlier an outlier. One can create a contribution plot for some given PC and some given observation (i.e. a beer batch). \n",
    "\n",
    "We want to compare how the contributions from an outlier differ from the contributions of in-spec batches. To do this, we need the batch label(s) for the outlier(s) and in-spec/reference batches.\n",
    "\n",
    "A contribution of an input feature to a PC for a given observation is the value of the input feature at that observation times its coefficient (element of eignevector for the PC).\n",
    "\n",
    "To find the batch labels, hover over the data points in the 3-D scatter plot found in Part 5d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a brand to analyze\n",
    "brand = \"Red Wonder\"\n",
    "\n",
    "# identify the batch label for outlier point(s) - has to be a list!\n",
    "outlier_batches = [37]  # outlier for \"Red Wonder\" brand\n",
    "\n",
    "############### >>>>> DO NOT CHANGE SECTION - START >>>>> #############\n",
    "# filter out irrelevant brands\n",
    "all_brand_batches = FeatureDF[FeatureDF[\"Brand\"] == brand_dictionary[brand]]\n",
    "\n",
    "# identify labels for in-spec batches\n",
    "inspec_batches = [\n",
    "    x for x in all_brand_batches[\"Batch\"].unique() if x not in outlier_batches\n",
    "]\n",
    "\n",
    "def extract_feature_values(ref_lbls, feat_matrix=X, feat_df=FeatureDF):\n",
    "    ref_idxs = []\n",
    "    for idx, lbl in enumerate(ref_lbls):\n",
    "        ref_idxs.append(feat_df.index[feat_df[\"Batch\"] == lbl].tolist()[0])\n",
    "    feature_values = list(feat_matrix[ref_idxs, :].mean(axis=0))\n",
    "    return feature_values\n",
    "\n",
    "def create_plot_object(\n",
    "    eigenvector,\n",
    "    feature_values,\n",
    "    bar_color,\n",
    "    feature_name,\n",
    "    show_legend=True,\n",
    "    feature_df=FeatureDF,\n",
    "    N_PCs=3,\n",
    "):\n",
    "    # compute feature contribution\n",
    "    contribution = [p * x for p, x in zip(eigenvector, feature_values)]\n",
    "    \n",
    "    # create plot object for the contribution\n",
    "    plot_object = go.Bar(\n",
    "        x=feature_df.columns.tolist()[2:],\n",
    "        y=contribution,\n",
    "        name=feature_name,\n",
    "        marker=dict(color=bar_color),\n",
    "        showlegend=show_legend,\n",
    "    )\n",
    "    return plot_object\n",
    "\n",
    "def plot_feature_contributions(\n",
    "    outlier_feature_vals, ref_feature_vals, eigenvector_matrix=P\n",
    "):\n",
    "    \n",
    "    # initialize contribution plots\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.03)\n",
    "    \n",
    "    # create plot objects for contributions in first 3 PCs\n",
    "    for i_component in range(3):\n",
    "        \n",
    "        # flag, whether to show legend\n",
    "        show_flag = True if i_component == 0 else False\n",
    "        \n",
    "        # eigenvector for PC\n",
    "        eigenvector = eigenvector_matrix[i_component, :]\n",
    "        \n",
    "        # create plot object for outlier batch(es)\n",
    "        outlier_obj = create_plot_object(\n",
    "            eigenvector, \n",
    "            outlier_feature_vals,\n",
    "            \"royalblue\",\n",
    "            \"Outlier\",\n",
    "            show_legend=show_flag,\n",
    "        )\n",
    "        \n",
    "        # create plot object for inspec batch(es)\n",
    "        ref_obj = create_plot_object(\n",
    "            eigenvector, \n",
    "            ref_feature_vals, \n",
    "            \"lightsteelblue\",\n",
    "            \"In-spec\",\n",
    "            show_legend=show_flag,\n",
    "        )\n",
    "        \n",
    "        # create bar plot for PC contribution\n",
    "        fig.append_trace(outlier_obj, i_component+1, 1)\n",
    "        fig.append_trace(ref_obj, i_component+1, 1)\n",
    "        \n",
    "        # label y axis appropriately\n",
    "        fig[\"layout\"][f\"yaxis{i_component+1}\"].update(title=f\"PC{i_component+1} Contribution\")\n",
    "        \n",
    "    # layout of bar plot\n",
    "    fig[\"layout\"].update(width=800, height=900, xaxis=dict(tickangle=45))\n",
    "    \n",
    "    # show bar plot\n",
    "    fig.show()\n",
    "    return\n",
    "    return\n",
    "############### <<<<< DO NOT CHANGE SECTION - END <<<<< #############\n",
    "\n",
    "# call function extract feature values\n",
    "outlier_feature_values = extract_feature_values(outlier_batches)\n",
    "inspec_feature_values = extract_feature_values(inspec_batches)\n",
    "\n",
    "# call function to plot contributions\n",
    "plot_feature_contributions(outlier_feature_values, inspec_feature_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:**    \n",
    "\n",
    "1. Why do you think there are bad batches?\n",
    "2. Does it seem like there is equipment failure at some point?\n",
    "3. What would you do to determine the root cause of a process or equipment failure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6. Apply clustering to Principal Components (BONUS)\n",
    "---\n",
    "<a id=’section_6’></a>\n",
    "There are many clustering algorithms which are available. One of the most commonly used is **k-means clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Apply k-means clustering on the principal components\n",
    "<a id=’section_6a’></a>\n",
    "We use the cumulative variance plot to decide on an appropriate number (\"most dominant\") of principal components, and we perform k-means clustering only those components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base this on CPV plot\n",
    "n_principal_components = 4\n",
    "\n",
    "# number of clusters = number of available beer brands\n",
    "K = len(FeatureDF[\"Brand\"].unique())\n",
    "\n",
    "# apply kmeans on principal components\n",
    "kmeans = KMeans(n_clusters=K).fit(T[:, :n_principal_components])\n",
    "\n",
    "# find which cluster each beer batch belongs to\n",
    "cluster_id = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Plot principal components and color by kmeans cluster\n",
    "<a id=’section_6b’></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data for all clusters\n",
    "obj_list = []\n",
    "\n",
    "all_clusters = list(set(cluster_id))\n",
    "for i_cluster, cluster in enumerate(all_clusters):\n",
    "\n",
    "    boolean = cluster_id == cluster\n",
    "\n",
    "    # create plotly object\n",
    "    plot_obj = go.Scatter3d(\n",
    "        x=T[boolean, 0],\n",
    "        y=T[boolean, 1],\n",
    "        z=T[boolean, 2],\n",
    "        text=list(FeatureDF.loc[boolean, \"Batch\"]),\n",
    "        hoverinfo=\"text\",\n",
    "        name=f\"Cluster {i_cluster+1}\",\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=7, opacity=0.75),\n",
    "    )\n",
    "    \n",
    "    # append object to list\n",
    "    obj_list.append(plot_obj)\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"PC 1\"), yaxis=dict(title=\"PC 2\"), zaxis=dict(title=\"PC 3\")\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=obj_list, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

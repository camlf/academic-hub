{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Time-Series-Analysis:-Predicting-the-Apparent-Degree-of-Fermentation-(ADF)\" data-toc-modified-id=\"Introduction-to-Time-Series-Analysis:-Predicting-the-Apparent-Degree-of-Fermentation-(ADF)-1\">Introduction to Time-Series Analysis: Predicting the Apparent Degree of Fermentation (ADF)</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Goal\" data-toc-modified-id=\"The-Goal-1.1\">The Goal</a></span></li><li><span><a href=\"#This-Notebook\" data-toc-modified-id=\"This-Notebook-1.2\">This Notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1.-Define-system-parameters\" data-toc-modified-id=\"Part-1.-Define-system-parameters-1.2.1\">Part 1. Define system parameters</a></span></li><li><span><a href=\"#Part-2.-Acquire-brewery-data\" data-toc-modified-id=\"Part-2.-Acquire-brewery-data-1.2.2\">Part 2. Acquire brewery data</a></span></li><li><span><a href=\"#Part-3.-Create-utility-functions-for-visualizing-data\" data-toc-modified-id=\"Part-3.-Create-utility-functions-for-visualizing-data-1.2.3\">Part 3. Create utility functions for visualizing data</a></span></li><li><span><a href=\"#Part-4.-Pre-process-brewery-data\" data-toc-modified-id=\"Part-4.-Pre-process-brewery-data-1.2.4\">Part 4. Pre-process brewery data</a></span></li><li><span><a href=\"#Part-5.-Fitting-and-analysis\" data-toc-modified-id=\"Part-5.-Fitting-and-analysis-1.2.5\">Part 5. Fitting and analysis</a></span></li></ul></li><li><span><a href=\"#Your-task\" data-toc-modified-id=\"Your-task-1.3\">Your task</a></span></li><li><span><a href=\"#Part-1.-Define-system-parameters\" data-toc-modified-id=\"Part-1.-Define-system-parameters-1.4\">Part 1. Define system parameters</a></span></li><li><span><a href=\"#Part-2.-Acquire-data-from-Deschutes-Brewery\" data-toc-modified-id=\"Part-2.-Acquire-data-from-Deschutes-Brewery-1.5\">Part 2. Acquire data from Deschutes Brewery</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-2b.-Download-data-with-Data-Views\" data-toc-modified-id=\"Part-2b.-Download-data-with-Data-Views-1.5.1\">Part 2b. Download data with data views</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-predefined-Data-Views\" data-toc-modified-id=\"Find-predefined-Data-Views-1.5.1.1\">Find predefined data views</a></span></li><li><span><a href=\"#Data-View-Structure-for-Fermenter-Vessels\" data-toc-modified-id=\"Data-View-Structure-for-Fermenter-Vessels-1.5.1.2\">data view Structure for Fermenter Vessels</a></span></li><li><span><a href=\"#Get-Interpolated-Data-from-Data-View\" data-toc-modified-id=\"Get-Interpolated-Data-from-Data-View-1.5.1.3\">Get Interpolated Data from data view</a></span></li><li><span><a href=\"#Verify-Data-Ordering-of-Results\" data-toc-modified-id=\"Verify-Data-Ordering-of-Results-1.5.1.4\">Verify Data Ordering of Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-3.-Create-utility-functions-for-visualizing-data\" data-toc-modified-id=\"Part-3.-Create-utility-functions-for-visualizing-data-1.6\">Part 3. Create utility functions for visualizing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-3a.-Function-for-previewing-data\" data-toc-modified-id=\"Part-3a.-Function-for-previewing-data-1.6.1\">Part 3a. Function for previewing data</a></span></li><li><span><a href=\"#Part-3b.-Function-for-plotting-fit-and-normality\" data-toc-modified-id=\"Part-3b.-Function-for-plotting-fit-and-normality-1.6.2\">Part 3b. Function for plotting fit and normality</a></span></li></ul></li><li><span><a href=\"#Part-4.-Pre-process-brewery-data\" data-toc-modified-id=\"Part-4.-Pre-process-brewery-data-1.7\">Part 4. Pre-process brewery data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-4a.-Filter-out-data-with-known-bad-values\" data-toc-modified-id=\"Part-4a.-Filter-out-data-with-known-bad-values-1.7.1\">Part 4a. Filter out data with known bad values</a></span></li><li><span><a href=\"#Part-4b.-Preview-raw-ADF-profile\" data-toc-modified-id=\"Part-4b.-Preview-raw-ADF-profile-1.7.2\">Part 4b. Preview raw ADF profile</a></span></li><li><span><a href=\"#Part-4c.-Create-smoothed-ADF-profile\" data-toc-modified-id=\"Part-4c.-Create-smoothed-ADF-profile-1.7.3\">Part 4c. Create smoothed ADF profile</a></span></li><li><span><a href=\"#Part-4d.-Preview-smooth-ADF-profile\" data-toc-modified-id=\"Part-4d.-Preview-smooth-ADF-profile-1.7.4\">Part 4d. Preview smooth ADF profile</a></span></li></ul></li><li><span><a href=\"#Part-5.-Fitting-and-analysis\" data-toc-modified-id=\"Part-5.-Fitting-and-analysis-1.8\">Part 5. Fitting and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-5a.-Fitting-to-a-linear-model\" data-toc-modified-id=\"Part-5a.-Fitting-to-a-linear-model-1.8.1\">Part 5a. Fitting to a linear model</a></span></li><li><span><a href=\"#Part-5b.-Fitting-to-a-piecewise-linear-model\" data-toc-modified-id=\"Part-5b.-Fitting-to-a-piecewise-linear-model-1.8.2\">Part 5b. Fitting to a piecewise linear model</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "# Introduction to Time-Series Analysis: Predicting the Apparent Degree of Fermentation (ADF)\n",
    "---\n",
    "The Apparent Degree of Fermentation (ADF) is one of the critical parameters monitored during the beer manufacturing processes. Deschutes Brewery previously obtained the ADF values based on the manual sampling of the specific gravity. The specific gravity is the relative density of the solution to the water density. Given the specific gravity, ADF is calculated as follows:\n",
    "\n",
    "$$\\text{Apparent Degree of Fermentaion} = \\frac{\\text{Starting Specific  Gravity - Current Specific Gravity}} {\\text{Starting Specific Gravity - 1}}$$\n",
    " \n",
    "The ADF is a crucial index when determining the transition point from the \"Fermentation\" phase to the \"Free Rise\" phase. The brewery should have the capability to detect the transition point to maintain consistent beer quality and release the product on time.\n",
    "\n",
    "The ADF can be measured either manually or with an automated ADF sensor. Using an automated ADF sensor requires a high capital investment. The manual sampling of ADF on the other hand is affordable but laborious. \n",
    "\n",
    "However, the brewery can enhance manual measurements of ADF by using them to create a predictive model. By creating such  predictive models, Deschutes Brewery saved $750k in capital investments by opting out of automated ADF sensors and reducing the operation time by being able to predict transition points.\n",
    "\n",
    "In this notebook, students will use real process data from Deschutes Brewery to create and evalutate models for predicting the ADF.\n",
    "\n",
    "First, make sure that all required libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ocs_academic_hub\n",
    "except ImportError:\n",
    "    !pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ocs-academic-hub==0.78.0\n",
    "    import ocs_academic_hub\n",
    "    \n",
    "# install piecewise linear functions for fitting        \n",
    "!pip install plotly_express pwlf sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import all the required libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic Hub module\n",
    "from ocs_academic_hub import HubClient\n",
    "\n",
    "# For OCS configuration and data manipulation\n",
    "import configparser\n",
    "from dateutil import parser\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# packages for plotting\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# analysis packages\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import pwlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Goal\n",
    "The Apparent Degree of Fermentation or ADF is an important indicator of the status of the fermentation. It indicates how much sugar has been converted into alcohol and could be used to predict transitions from the \"Fermentation\" phase to the \"Free Rise\" phase.\n",
    "\n",
    "The ADF is computed from the specific gravity. However, the specific gravity is measured manually through a laborious process which is also performed intermittently.\n",
    "\n",
    "**In this notebook, we'll use real process data from Deschutes Brewery to create and evaluate predictive models for ADF.**\n",
    "\n",
    "Such models could be used to reduce operation time by predicting transitions in the brewery process, thus augmenting the value of manual ADF measurements and reducing the need for making costly capital investments like automated sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## This Notebook\n",
    "\n",
    "### Part 1. Define system parameters\n",
    "User specifies time period, time granularity, fermentor vessels, brand of interest, and other parameters.\n",
    "\n",
    "### Part 2. Acquire brewery data\n",
    "Use OSIsoft Cloud Services (OCS) to obtain real process data from Deschutes Brewery ([2a](#section_2a)) and store it into a dataframe ([2b](#section_2b)).\n",
    "\n",
    "### Part 3. Create utility functions for visualizing data\n",
    "Create functions for previewing the ADF profile ([3a](#section_3a)); and plotting the ADF prediction model, residuals, and normality tests ([3b](#section_3b)).\n",
    "\n",
    "### Part 4. Pre-process brewery data\n",
    "Pre-process the brewery data before fitting and analysis. Remove obviously bad data points and unnecessary attributes ([4a](#section_4a)); identify fermentation batches and compute the duration of fermentation ([4c](#section_4c)); manually remove emergent outliers ([4e](#section_4e)).\n",
    "\n",
    "### Part 5. Fitting and analysis\n",
    "Fit the cleaned up data frame to a linear model ([5a](#section_5a)) and then to a piecewise linear model ([5b](#section_5b)). Evaluate and compare the model fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Your task\n",
    "Some cells in [Part 4](#section_4) have missing code and contain `TODO` items in comments. \n",
    "\n",
    "Fill in all the missing code to obtain a working notebook. If the notebook runs as intended, one should obtain the following plot for the analysis of the linear model in [Part 5a](#section_5a) and an analogous plot for the piecewise linear fit in [Part 5b](#section_5b):\n",
    "\n",
    "![](https://academichub.blob.core.windows.net/images/nb1_linear_reg_v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Define system parameters\n",
    "<a id='section_1'></a>\n",
    "User specifies time period, time granularity, fermentor vessels and brand of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant days over which to analyze fermentors\n",
    "TRAINING_DAYS = 365\n",
    "\n",
    "# time period in which to analyze fermentation data\n",
    "START_INDEX = \"2017-01-11T00:00\"\n",
    "END_INDEX = (parser.parse(START_INDEX) + dt.timedelta(days=TRAINING_DAYS)).isoformat()\n",
    "print(f\"# Date range: from {START_INDEX} to {END_INDEX}\")\n",
    "\n",
    "# define time granularity for data. in this case, get events at 10 minute intervals\n",
    "INDEX_INTERVAL = \"00:10:00\"\n",
    "\n",
    "# brand of interest\n",
    "BRAND_OF_INTEREST = \"Realtime Hops\"  # \"Kerberos\"  \n",
    "\n",
    "\n",
    "# (Optional) List of known bad Fermentation IDs to remove from data\n",
    "BAD_FERM_IDS = []  # e.g. [\"FV31_20170209\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Part 2. Acquire data from Deschutes Brewery\n",
    "<a id='section_2'></a>\n",
    "\n",
    "---\n",
    "**Executing the next starts the authentication process for your Google account, acting as the Identity Provider for OCS. At the end of a successful login or if you're already logged in with Google, you'll see the following message on a new browser tab:**\n",
    "\n",
    "**You can now return to the application.**\n",
    "\n",
    "Simply close this tab and return to your notebook tab. \n",
    "\n",
    "---\n",
    "\n",
    "After a successful login, communication with OCS is done through a HubClient object. Variable `namespace_id` is initialized with the OCS namespace hosting the Deschutes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_client = HubClient()\n",
    "\n",
    "dataset = hub_client.current_dataset()\n",
    "namespace_id = hub_client.namespace_of(dataset)\n",
    "print(f\"Current dataset is {dataset} in namespace_id: {namespace_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b. Download data with data views \n",
    "<a id='section_2b'></a>\n",
    "\n",
    "The Deschutes dataset is equipped with a set of pre-defined data views to easily extract data related to one asset or more. In this notebook the data of interest is coming from fermenter vessels. The convention for fermenter vessels in this dataset is to have an Asset ID of the format `FV<num>` where `<num>` is a 2 digit number. \n",
    "fermenter vessel or more. Each data view is identified by a unique identifier (ID) to be provided when making a requests to get actual data from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find assets and their predefined Data Views \n",
    "\n",
    "First to get a list of assets based on a filter, here for fermenter vessels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_client.assets(filter=\"FV3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the list of pre-defined multi-asset (with `single_asset=False`) data views for \"ADF\" analysis. We'll see below the actual definition of this data view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of data view IDs\n",
    "dv_ids = hub_client.asset_dataviews(filter=\"adf\", multiple_asset=True)\n",
    "dv_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### data view Structure for Fermenter Vessels\n",
    "\n",
    "Each data view has a set of columns which can be one of the follwowing types: \n",
    "\n",
    "* Float for numerical data\n",
    "* Timestamp for date/time data \n",
    "* Integer \n",
    "* String \n",
    "* Category (digital states)  \n",
    "\n",
    "The method `dataview_definition` returns a dataframe with a data view structure represented by the following columns:\n",
    "\n",
    "* `Asset_Id`: asset identification attached to the row\n",
    "* `OCS_StreamName`: actual OCS stream providing data for this column\n",
    "* `DV_Column`: name of the column (header)\n",
    "* `Value_Type`: type of the value found in the column\n",
    "* `EngUnits`: declared engineering units (if any) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data view here is a multi-asset one\n",
    "print(hub_client.dataview_definition(namespace_id, dv_ids[0]).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Get Interpolated Data from Data View\n",
    "\n",
    "To use a data view one needs:\n",
    "\n",
    "* namespace hosting the data view\n",
    "* its ID\n",
    "* start time (also called index since OCS supports non-time indexes)\n",
    "* end time\n",
    "* interval (of interpolation)\n",
    "\n",
    "Data views return data into CSV (Comma Separated Values) format. The Academic Hub module provides utility functions like `dataviews_interpolated_pd` that returns a ready-to-use Panda dataframe built from those CSVs. Each row starts with a timestamp, followed by the Asset ID of the asset to which the remaining data view result row belong to. \n",
    "\n",
    "**Note: it may take up to 30 seconds to complete depending on the speed of your connection and computer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brands_df = hub_client.dataview_interpolated_pd(\n",
    "    namespace_id, dv_ids[0], START_INDEX, END_INDEX, INDEX_INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary information about the dataframe\n",
    "all_brands_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first five rows \n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brands_df[all_brands_df[\"Status\"] == \"Fermentation\"][\"Brand\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Verify Data Ordering of Results\n",
    "\n",
    "The result rows of a multi-asset data view are sorted first on `Asset_Id`, then on `Timestamp`. The Pandas line below shows the first 3 rows of each asset and their index in the dataframe (left hand side integer in bold). Since the request was for 395 days at 10 minutes interval, we expect 365 * 24 * 60 / 10 + 1 = 52561 rows per fermenter vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brands_df[all_brands_df[\"Asset_Id\"].shift(3) != all_brands_df[\"Asset_Id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Create utility functions for visualizing data\n",
    "<a id='section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3a. Function for previewing data\n",
    "<a id='section_3a'></a>\n",
    "\n",
    "The *known* bad values have been eliminated from our dataset. However, there may still exist bad values which are *unknown*.\n",
    "\n",
    "A simple way to identify possible bad values is to visualize the data by plotting them. So we create the function `plot_preview_data` to accomplish this. \n",
    "\n",
    "NOTE: We want `plot_preview_data` to be such that it can be re-used to visualize data at any point in the mutiple data cleansing steps that will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preview_data(\n",
    "    df, X_attribute, Y_attribute, all_group_ids, layout, mode=\"lines+markers\"\n",
    "):\n",
    "\n",
    "    # create list for plotly objects\n",
    "    plot_objects = []\n",
    "\n",
    "    # go through each fermentor vessel\n",
    "    for group_id in all_group_ids:\n",
    "\n",
    "        # create boolean for extracting data for time and vessels of interest\n",
    "        group_boolean = df[\"Fermentation ID\"] == group_id\n",
    "\n",
    "        # labels for plotting\n",
    "        name = group_id\n",
    "\n",
    "        # create plotly object\n",
    "        plot_object = go.Scatter(\n",
    "            x=df.loc[group_boolean, X_attribute],\n",
    "            y=df.loc[group_boolean, Y_attribute],\n",
    "            mode=mode,\n",
    "            name=name,\n",
    "            hoverlabel={\"namelength\": -1},\n",
    "        )\n",
    "\n",
    "        # store plotly objects into list\n",
    "        plot_objects.append(plot_object)\n",
    "\n",
    "    # create and show plot\n",
    "    fig = go.Figure(data=plot_objects, layout=layout)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3b. Function for plotting fit and normality\n",
    "<a id='section_3b'></a>\n",
    "Create a function `model_analysis_plot` which  (1) plots the observed data and the model fit, (2) computes and plots residuals, and (3) computes and plots the normal scores for evaluating the quality of the model.\n",
    "\n",
    "`model_analysis_plot` should be re-useable, to easily plot and evaluate any model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis_plot(X, Y_observed, Y_model, model_name):\n",
    "\n",
    "    # compute residuals\n",
    "    residuals = Y_observed - Y_model\n",
    "\n",
    "    # normal probabilities\n",
    "    qq = stats.probplot(residuals)\n",
    "    normal_line = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "\n",
    "    # initialize subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{\"colspan\": 2}, None], [{}, {}]],\n",
    "        vertical_spacing=0.10,\n",
    "        horizontal_spacing=0.03,\n",
    "        shared_yaxes=True,\n",
    "    )\n",
    "\n",
    "    # create plot object for observed data\n",
    "    obj_observed = go.Scatter(\n",
    "        x=X, y=Y_observed, mode=\"markers\", marker=dict(color=\"black\"), name=\"observed\"\n",
    "    )\n",
    "\n",
    "    # create plot object for model data\n",
    "    obj_model = go.Scatter(\n",
    "        x=X, y=Y_model, mode=\"lines\", line=dict(color=\"red\"), name=f\"{model_name} model\"\n",
    "    )\n",
    "\n",
    "    # create plot object for residuals data\n",
    "    obj_residuals = go.Scatter(\n",
    "        x=Y_model,\n",
    "        y=residuals,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"grey\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # create plot normal scores of model residuals\n",
    "    obj_residual_scores = go.Scatter(\n",
    "        x=qq[0][0],\n",
    "        y=qq[0][1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"grey\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # create plot for normal scores of normally distributed data\n",
    "    obj_normal = go.Scatter(\n",
    "        x=normal_line,\n",
    "        y=qq[1][1] + qq[1][0] * normal_line,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # add plot objects to figure\n",
    "    fig.append_trace(obj_observed, row=1, col=1)\n",
    "    fig.append_trace(obj_model, row=1, col=1)\n",
    "    fig.append_trace(obj_residuals, row=2, col=1)\n",
    "    fig.append_trace(obj_residual_scores, row=2, col=2)\n",
    "    fig.append_trace(obj_normal, row=2, col=2)\n",
    "\n",
    "    # update x axis properties\n",
    "    fig.layout.xaxis1.update(title=\"Fermentation time (hour)\")\n",
    "    fig.layout.xaxis2.update(title=\"Predicted Values\")\n",
    "    fig.layout.xaxis3.update(title=\"Normal Scores\")\n",
    "\n",
    "    # update y axis properties\n",
    "    fig.layout.yaxis1.update(title=\"ADF\")\n",
    "    fig.layout.yaxis2.update(title=\"Residuals\")\n",
    "\n",
    "    # update plot layout\n",
    "    fig.layout.update(height=820)\n",
    "    fig.layout.update(width=900)\n",
    "\n",
    "    # show plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Part 4. Pre-process brewery data\n",
    "<a id='section_4'></a>\n",
    "To create the ADF prediction curve, we need ADF vs time data for the `BRAND_OF_INTEREST`. The ADF profile for a brand (assuming none are bad batches) should look almost identical regardless of when it was fermented and at which vessel it was fermented.\n",
    "\n",
    "Thus in order to create a model that best predicts the ADF profile for some given brand, outliers and bad batches have to be removed from the training set. And that is the goal for this section.\n",
    "\n",
    "Some of the bad batches and bad data points are known beforehand, but some have to be identified graphically and manually removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4a. Filter out data with known bad values\n",
    "<a id='section_4a'></a>\n",
    "Some of the bad batches (`BAD_FERM_IDS`) in this time period is known beforehand. When sensors fail no value will be recorded at one or more timestamps. The reason for missing sensor values are multiple: network error, automation system glitch, hardware failure, etc. A digital code describing the error is recorded at the same timestamp of the missing value, but they are not exposed by the predefined fermenter vessel data views because there is nothing you can knowing this code. What we want to filter out the rows with missing values. \n",
    "\n",
    "Additionally, we want to filter out all unwanted values (e.g. data during non-fermentation stages will not add any useful information to the ADF profile). We also do not want to keep any attributes that will not help in creating a model for the ADF profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brands_df = all_brands_df.astype({\"Fermentation Start Time\": \"datetime64\"})\n",
    "\n",
    "\n",
    "def fermentation_id(fv_id, t0):\n",
    "    tt = str(t0).split(\" \")\n",
    "    return f\"{fv_id}_{tt[0].replace('-','')}\"\n",
    "\n",
    "\n",
    "all_brands_df[\"Fermentation ID\"] = all_brands_df.apply(\n",
    "    lambda row: fermentation_id(row[\"Asset_Id\"], row[\"Fermentation Start Time\"]), axis=1\n",
    ")\n",
    "\n",
    "# keep only data for the brand of interest\n",
    "brand_df = all_brands_df[all_brands_df[\"Brand\"] == BRAND_OF_INTEREST]\n",
    "\n",
    "# we are only interested in events where status is \"Fermentation\"\n",
    "# TODO: complete the filter expression to keep only data corresponding to the fermentation stages\n",
    "# =========== STUDENT BEGIN ==========\n",
    "brand_df = brand_df[@@@ Your code here @@@]\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "\n",
    "# Replace missing ADF value at beginning of fermentation by value 0.0\n",
    "brand_df.loc[np.isnan(brand_df[\"ADF\"]), \"ADF\"] = 0.0\n",
    "\n",
    "# No need for those two columns from which ADF is derived\n",
    "brand_df = brand_df.drop(columns=[\"FV Full Plato\", \"Plato\"])\n",
    "\n",
    "# remove all rows with missing values\n",
    "# TODO: complete expression, hint check PandasGuide.ipynb section \"Selection\"\n",
    "# =========== STUDENT BEGIN ==========\n",
    "brand_df = @@@ Your code here @@@\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "\n",
    "# remove all known bad fermentations\n",
    "# brand_df = brand_df[~brand_df[\"Fermentation ID\"].isin(BAD_FERM_IDS)]\n",
    "\n",
    "# preview first few lines of dataframe\n",
    "brand_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4b. Preview raw ADF profile\n",
    "<a id='section_4b'></a>\n",
    "Preview what the data looks like for the first `N_days`. Plot ADF vs the dates. You will observe that the ADF profile for a batch is discontinous. This is because the ADF has been measured manually and therefore intermittently.\n",
    "\n",
    "We ultimately want a more continuous ADF profile for fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time period of interest for preview\n",
    "N_days = TRAINING_DAYS\n",
    "\n",
    "# time after Ndays\n",
    "end_time = (parser.parse(START_INDEX) + dt.timedelta(days=N_days)).isoformat()\n",
    "\n",
    "# get all fermentation IDs (corresponds to different batches)\n",
    "# TODO: extract all unique Fermentation ID, hint: section \"Selection\" in PandasGuide.ipynb\n",
    "# =========== STUDENT BEGIN ==========\n",
    "all_ferm_ids = sorted(@@@ You code here @@@)\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "\n",
    "# define layout of plot\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Timestamp (days)\"),\n",
    "    yaxis=dict(title=\"ADF\"),\n",
    "    width=800,\n",
    "    title=f\"Preview of ADF during fermentation stage, first {N_days} days starting on {START_INDEX} \"\n",
    "    f\"(total of {len(all_ferm_ids)} fermentation in {TRAINING_DAYS} days)\",\n",
    ")\n",
    "\n",
    "# plot preview of data\n",
    "plot_preview_data(\n",
    "    brand_df[brand_df[\"Timestamp\"] <= end_time],\n",
    "    \"Timestamp\",\n",
    "    \"ADF\",\n",
    "    all_ferm_ids,\n",
    "    layout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4c. Create smoothed ADF profile\n",
    "<a id='section_4c'></a>\n",
    "In order to create a predictive model for ADF, the start of fermentation and the fermentation times should be identified. It is known that the total duration of the fermentation phase does not exceed 2.5 days, and that at the start of fermentation, the ADF should approximately equal zero.\n",
    "\n",
    "Furthermore, since ADF is measured intermittently, keep only events (rows) that correspond to new ADF measurements. This should result in a more continuous (smooth) ADF profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only entries that correspond to new ADF measurements (i.e. previous ADF does not match current ADF)\n",
    "# within the same batch identified by \"Fermentation ID\"\n",
    "brand_df = brand_df[\n",
    "    (brand_df[\"ADF\"].shift(1) != brand_df[\"ADF\"])\n",
    "    & (brand_df[\"Fermentation ID\"] == brand_df[\"Fermentation ID\"])\n",
    "].reset_index()\n",
    "\n",
    "\n",
    "# arrange data into chronological order\n",
    "brand_df.sort_values(by=[\"Fermentation ID\", \"Timestamp\"], inplace=True)\n",
    "\n",
    "\n",
    "# find the start of fermentation (ADF ~= 0.0)\n",
    "fermentation_starts = brand_df[abs(brand_df[\"ADF\"]) <= 0.000001].reset_index()\n",
    "fermentation_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe column for recording the time evolution of fermentation\n",
    "brand_df[\"Elapsed\"] = -1\n",
    "\n",
    "# compute the time elapsed since the starts of a fermentation\n",
    "for count, fermentation_start_row in fermentation_starts.iterrows():\n",
    "\n",
    "    # get the timestamp for each start of fermentation\n",
    "    fermentation_start_time = fermentation_start_row[\"Timestamp\"]\n",
    "    fermentation_id = fermentation_start_row[\"Fermentation ID\"]\n",
    "\n",
    "    # boolean vector to pick only rows relevant to current fermentation ID\n",
    "    batch_row_picker = brand_df[\"Fermentation ID\"] == fermentation_id\n",
    "\n",
    "    # find time elapsed since the start of fermentation in units of hours\n",
    "    brand_df.loc[batch_row_picker, \"Elapsed\"] = brand_df.loc[\n",
    "        batch_row_picker, \"Timestamp\"\n",
    "    ].apply(lambda t: (t - fermentation_start_time).total_seconds() / 3600.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all batches with wrong elapsed time \n",
    "brand_df = brand_df[~(brand_df[\"Elapsed\"] < 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data from t=0 to t=tmax\n",
    "brand_df = brand_df.sort_values(by=\"Elapsed\")\n",
    "\n",
    "# preview dataframe\n",
    "brand_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4d. Preview smooth ADF profile\n",
    "<a id='section_4d'></a>\n",
    "Preview the smoothed ADF profile. Instead of plotting it with respect to the timestamp, plot it with respect to the elapsed time of fermentation computed in Part 4c. To perform the plotting, we re-use the function `plot_preview_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferm_ids = sorted(brand_df[\"Fermentation ID\"].unique())\n",
    "\n",
    "# define layout of plot\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Fermentation Time (hours)\"),\n",
    "    yaxis=dict(title=\"ADF\"),\n",
    "    width=800,\n",
    "    title=f\"ADF -- {BRAND_OF_INTEREST} brand, {len(ferm_ids)} fermentations, {START_INDEX} - {END_INDEX}\",\n",
    ")\n",
    "\n",
    "# plot preview of data\n",
    "plot_preview_data(brand_df, \"Elapsed\", \"ADF\", ferm_ids, layout, mode=\"markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Part 5. Fitting and analysis\n",
    "<a id='section_5e'></a>\n",
    "Fit the data to a model and then asses the quality of that fit by looking at plots of the residuals (qualitative) and the normal scores (quantitative). We consider two models, linear and piecewise linear.\n",
    "\n",
    "For the linear model, we have model parameters $\\alpha$ and $\\beta$. The $\\epsilon$ is the difference between the observed values and the model predictions. If the model is appropriate, the $\\epsilon$ should be uncorrelated with each other.\n",
    "\n",
    "$Y = \\alpha + \\beta X + \\epsilon$\n",
    "\n",
    "The piecewise linear function we are considering has three breaks and 9 model parameters.\n",
    "\n",
    "$Y = \n",
    "    \\begin{cases}\n",
    "    \\alpha_1 + \\beta_1 X + \\epsilon, & X \\leq \\gamma_1 \\\\\n",
    "    \\alpha_2 + \\beta_2 X + \\epsilon, & \\gamma_1 \\leq X \\leq \\gamma_2 \\\\\n",
    "    \\alpha_3 + \\beta_3 X + \\epsilon, & \\gamma_2 \\leq X\n",
    "    \\end{cases}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5a. Fitting to a linear model\n",
    "<a id='section_5a'></a>\n",
    "Define the training set, apply linear regression on the training set. Compare the model fit with observations, and gauge the quality of the model fit by looking at a plot of the residuals. The residuals should not have a trend (i.e. are normally distributed). One way to test whether a process is normally distributed is through a \"normality test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before any fitting can be performed, the training set has to be defined\n",
    "X_observed = brand_df[\"Elapsed\"].to_frame()\n",
    "Y_observed = brand_df[\"ADF\"].to_frame()\n",
    "\n",
    "# create the linear model. use existing libraries\n",
    "\n",
    "# Create the linear model using X_observed and Y_observed. Use the scikit learn library and follow this example: \n",
    "#                 https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "# =========== STUDENT BEGIN ==========\n",
    "\n",
    "\n",
    "Y_linear =  \n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "\n",
    "# graphically compoar observed data vs linear model\n",
    "model_analysis_plot(X_observed[\"Elapsed\"], Y_observed[\"ADF\"], Y_linear, \"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5b. Fitting to a piecewise linear model\n",
    "<a id='section_5b'></a>\n",
    "Perform peicewise linear regression on the training set defined previously. Graphically compare the model fit with the observations; analyze the residuals and the results for the normality test.\n",
    "\n",
    "Our piecewise linear model has three breaks corresponding to the \"lag phase\", \"growth phase\", and \"fermentation phase\" in a normal brewery fermentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the piecewise linear model\n",
    "my_pwlf = pwlf.PiecewiseLinFit(X_observed[\"Elapsed\"], Y_observed[\"ADF\"])\n",
    "breaks = my_pwlf.fit(3)\n",
    "Y_pwlf = my_pwlf.predict(X_observed[\"Elapsed\"])\n",
    "\n",
    "# graphically compare observed data vs piecewise linear model\n",
    "model_analysis_plot(X_observed[\"Elapsed\"], Y_observed[\"ADF\"], Y_pwlf, \"Piecewise Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:**\n",
    "\n",
    "1. Which of the models used in this notebook performed the best and why?\n",
    "2. Can you think of a different model that would perform better than either of the models used in this notebook?\n",
    "3. Are there any trade-offs to using more complex models? If so, what are they?\n",
    "4. Rerun this notebook but with beer brand Kerberos (instead of Realtime Hops). You will need to identify bad fermentation IDs and add them to list BAD_FERM_IDS to perform the regressions if need be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "569px",
    "width": "680px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Time-Series-Analysis:-Predicting-the-Apparent-Degree-of-Fermentation-(ADF)\" data-toc-modified-id=\"Introduction-to-Time-Series-Analysis:-Predicting-the-Apparent-Degree-of-Fermentation-(ADF)-1\">Introduction to Time-Series Analysis: Predicting the Apparent Degree of Fermentation (ADF)</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Goal\" data-toc-modified-id=\"The-Goal-1.1\">The Goal</a></span></li><li><span><a href=\"#This-Notebook\" data-toc-modified-id=\"This-Notebook-1.2\">This Notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1.-Define-system-parameters\" data-toc-modified-id=\"Part-1.-Define-system-parameters-1.2.1\">Part 1. Define system parameters</a></span></li><li><span><a href=\"#Part-2.-Acquire-brewery-data\" data-toc-modified-id=\"Part-2.-Acquire-brewery-data-1.2.2\">Part 2. Acquire brewery data</a></span></li><li><span><a href=\"#Part-3.-Create-utility-functions-for-visualizing-data\" data-toc-modified-id=\"Part-3.-Create-utility-functions-for-visualizing-data-1.2.3\">Part 3. Create utility functions for visualizing data</a></span></li><li><span><a href=\"#Part-4.-Pre-process-brewery-data\" data-toc-modified-id=\"Part-4.-Pre-process-brewery-data-1.2.4\">Part 4. Pre-process brewery data</a></span></li><li><span><a href=\"#Part-5.-Fitting-and-analysis\" data-toc-modified-id=\"Part-5.-Fitting-and-analysis-1.2.5\">Part 5. Fitting and analysis</a></span></li></ul></li><li><span><a href=\"#Your-task\" data-toc-modified-id=\"Your-task-1.3\">Your task</a></span></li><li><span><a href=\"#Part-1.-Define-system-parameters\" data-toc-modified-id=\"Part-1.-Define-system-parameters-1.4\">Part 1. Define system parameters</a></span></li><li><span><a href=\"#Part-2.-Acquire-data-from-Deschutes-Brewery\" data-toc-modified-id=\"Part-2.-Acquire-data-from-Deschutes-Brewery-1.5\">Part 2. Acquire data from Deschutes Brewery</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-2a.-Standard-OCS-Initialization\" data-toc-modified-id=\"Part-2a.-Standard-OCS-Initialization-1.5.1\">Part 2a. Standard OCS Initialization</a></span></li><li><span><a href=\"#Part-2b.-Download-data-with-Data-Views\" data-toc-modified-id=\"Part-2b.-Download-data-with-Data-Views-1.5.2\">Part 2b. Download data with Data Views</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-predefined-Data-Views\" data-toc-modified-id=\"Find-predefined-Data-Views-1.5.2.1\">Find predefined Data Views</a></span></li><li><span><a href=\"#Data-View-Structure-for-Fermenter-Vessels\" data-toc-modified-id=\"Data-View-Structure-for-Fermenter-Vessels-1.5.2.2\">Data View Structure for Fermenter Vessels</a></span></li><li><span><a href=\"#Get-Interpolated-Data-from-Data-View\" data-toc-modified-id=\"Get-Interpolated-Data-from-Data-View-1.5.2.3\">Get Interpolated Data from Data View</a></span></li><li><span><a href=\"#Verify-Data-Ordering-of-Results\" data-toc-modified-id=\"Verify-Data-Ordering-of-Results-1.5.2.4\">Verify Data Ordering of Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-3.-Create-utility-functions-for-visualizing-data\" data-toc-modified-id=\"Part-3.-Create-utility-functions-for-visualizing-data-1.6\">Part 3. Create utility functions for visualizing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-3a.-Function-for-previewing-data\" data-toc-modified-id=\"Part-3a.-Function-for-previewing-data-1.6.1\">Part 3a. Function for previewing data</a></span></li><li><span><a href=\"#Part-3b.-Function-for-plotting-fit-and-normality\" data-toc-modified-id=\"Part-3b.-Function-for-plotting-fit-and-normality-1.6.2\">Part 3b. Function for plotting fit and normality</a></span></li></ul></li><li><span><a href=\"#Part-4.-Pre-process-brewery-data\" data-toc-modified-id=\"Part-4.-Pre-process-brewery-data-1.7\">Part 4. Pre-process brewery data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-4a.-Filter-out-data-with-known-bad-values\" data-toc-modified-id=\"Part-4a.-Filter-out-data-with-known-bad-values-1.7.1\">Part 4a. Filter out data with known bad values</a></span></li><li><span><a href=\"#Part-4b.-Preview-raw-ADF-profile\" data-toc-modified-id=\"Part-4b.-Preview-raw-ADF-profile-1.7.2\">Part 4b. Preview raw ADF profile</a></span></li><li><span><a href=\"#Part-4c.-Create-smoothed-ADF-profile\" data-toc-modified-id=\"Part-4c.-Create-smoothed-ADF-profile-1.7.3\">Part 4c. Create smoothed ADF profile</a></span></li><li><span><a href=\"#Part-4d.-Preview-smooth-ADF-profile\" data-toc-modified-id=\"Part-4d.-Preview-smooth-ADF-profile-1.7.4\">Part 4d. Preview smooth ADF profile</a></span></li><li><span><a href=\"#Part-4e.-Manually-remove-outliers\" data-toc-modified-id=\"Part-4e.-Manually-remove-outliers-1.7.5\">Part 4e. Manually remove outliers</a></span></li></ul></li><li><span><a href=\"#Part-5.-Fitting-and-analysis\" data-toc-modified-id=\"Part-5.-Fitting-and-analysis-1.8\">Part 5. Fitting and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-5a.-Fitting-to-a-linear-model\" data-toc-modified-id=\"Part-5a.-Fitting-to-a-linear-model-1.8.1\">Part 5a. Fitting to a linear model</a></span></li><li><span><a href=\"#Part-5b.-Fitting-to-a-piecewise-linear-model\" data-toc-modified-id=\"Part-5b.-Fitting-to-a-piecewise-linear-model-1.8.2\">Part 5b. Fitting to a piecewise linear model</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "# Introduction to Time-Series Analysis: Predicting the Apparent Degree of Fermentation (ADF)\n",
    "---\n",
    "The Apparent Degree of Fermentation (ADF) is one of the critical parameters monitored during the beer manufacturing processes. Deschutes Brewery previously obtained the ADF values based on the manual sampling of the specific gravity. The specific gravity is the relative density of the solution to the water density. Given the specific gravity, ADF is calculated as follows:\n",
    "\n",
    "$$\\text{Apparent Degree of Fermentaion} = \\frac{\\text{Starting Specific  Gravity - Current Specific Gravity}} {\\text{Starting Specific Gravity - 1}}$$\n",
    " \n",
    "The ADF is a crucial index when determining the transition point from the \"Fermentation\" phase to the \"Free Rise\" phase. The brewery should have the capability to detect the transition point to maintain consistent beer quality and release the product on time.\n",
    "\n",
    "The ADF can be measured either manually or with an automated ADF sensor. Using an automated ADF sensor requires a high capital investment. The manual sampling of ADF on the other hand is affordable but laborious. \n",
    "\n",
    "However, the brewery can enhance manual measurements of ADF by using them to create a predictive model. By creating such  predictive models, Deschutes Brewery saved $750k in capital investments by opting out of automated ADF sensors and reducing the operation time by being able to predict transition points.\n",
    "\n",
    "In this notebook, students will use real process data from Deschutes Brewery to create and evalutate models for predicting the ADF.\n",
    "\n",
    "First, make sure that all required libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure most recent version of OCS hub is installed\n",
    "!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ocs-academic-hub==0.56.0\n",
    "    \n",
    "# install piecewise linear functions for fitting        \n",
    "!pip install plotly_express==0.4.1 pwlf sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import all the required libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic Hub module\n",
    "from ocs_academic_hub import HubClient, timer\n",
    "\n",
    "# For OCS configuration and data manipulation\n",
    "import configparser\n",
    "from dateutil import parser\n",
    "import datetime as dt\n",
    "\n",
    "# packages for plotting\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# analysis packages\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import pwlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Goal\n",
    "The Apparent Degree of Fermentation or ADF is an important indicator of the status of the fermentation. It indicates how much sugar has been converted into alcohol and could be used to predict transitions from the \"Fermentation\" phase to the \"Free Rise\" phase.\n",
    "\n",
    "The ADF is computed from the specific gravity. However, the specific gravity is measured manually through a laborious process which is also performed intermittently.\n",
    "\n",
    "**In this notebook, we'll use real process data from Deschutes Brewery to create and evaluate predictive models for ADF.**\n",
    "\n",
    "Such models could be used to reduce operation time by predicting transitions in the brewery process, thus augmenting the value of manual ADF measurements and reducing the need for making costly capital investments like automated sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## This Notebook\n",
    "\n",
    "### Part 1. Define system parameters\n",
    "User specifies time period, time granularity, fermentor vessels, brand of interest, and other parameters.\n",
    "\n",
    "### Part 2. Acquire brewery data\n",
    "Use OSIsoft Cloud Services (OCS) to obtain real process data from Deschutes Brewery ([2a](#section_2a)) and store it into a dataframe ([2b](#section_2b)).\n",
    "\n",
    "### Part 3. Create utility functions for visualizing data\n",
    "Create functions for previewing the ADF profile ([3a](#section_3a)); and plotting the ADF prediction model, residuals, and normality tests ([3b](#section_3b)).\n",
    "\n",
    "### Part 4. Pre-process brewery data\n",
    "Pre-process the brewery data before fitting and analysis. Remove obviously bad data points and unnecessary attributes ([4a](#section_4a)); identify fermentation batches and compute the duration of fermentation ([4c](#section_4c)); manually remove emergent outliers ([4e](#section_4e)).\n",
    "\n",
    "### Part 5. Fitting and analysis\n",
    "Fit the cleaned up data frame to a linear model ([5a](#section_5a)) and then to a piecewise linear model ([5b](#section_5b)). Evaluate and compare the model fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Your task\n",
    "Some cells in [Part 4](#section_4) have missing code and contain `TODO` items in comments. \n",
    "\n",
    "Fill in all the missing code to obtain a working notebook. If the notebook runs as intended, one should obtain the following plot for the analysis of the linear model in [Part 5a](#section_5a) and an analogous plot for the piecewise linear fit in [Part 5b](#section_5b):\n",
    "\n",
    "![](https://academicpi.blob.core.windows.net/images/NB1_Analyze_Fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Define system parameters\n",
    "<a id='section_1'></a>\n",
    "User specifies time period, time granularity, fermentor vessels and brand of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant days over which to analyze fermentors \n",
    "TRAINING_DAYS = 300\n",
    "\n",
    "# time period in which to analyze fermentation data\n",
    "START_INDEX = \"2016-01-17T00:00\"\n",
    "END_INDEX = (parser.parse(START_INDEX) + dt.timedelta(days=TRAINING_DAYS)).isoformat()\n",
    "\n",
    "# define time granularity for data. in this case, get events at 10 minute intervals\n",
    "INDEX_INTERVAL = \"00:10:00\"\n",
    "\n",
    "# brand of interest\n",
    "BRAND_OF_INTEREST = \"Realtime Hops\"\n",
    "\n",
    "# fermentor vessel of interest (multiple assets are available)\n",
    "FERMENTORS_OF_INTEREST = range(31, 37)\n",
    "\n",
    "# list of relevant attributes. these attributes should not have any bad inputs\n",
    "ATTRIBUTES_OF_INTEREST = [\"Timestamp\", \"ADF\",\"Dataview_ID\", \"Fermentation ID\"]\n",
    "\n",
    "# list of bad inputs\n",
    "BAD_INPUTS = [\"Calc Failed\"]\n",
    "\n",
    "# list of known bad fermentations\n",
    "BAD_FERM_IDS = [\n",
    "    \"Fermentor 362017423070\",\n",
    "    \"Fermentor 36201742557961\",\n",
    "]\n",
    "\n",
    "# EMERGENCIES ONLY! \n",
    "# If OCS not working, set to false \n",
    "USE_OCS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Part 2. Acquire data from Deschutes Brewery\n",
    "<a id='section_2'></a>\n",
    "Use OSIsoft Cloud Services (OCS) to load real process data from Deschutes Brewery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a. Standard OCS Initialization\n",
    "<a id='section_2a'></a>\n",
    "\n",
    "---\n",
    "**REQUIRED: a file `config.ini` should have been provided to you and put in the same directory as this notebook. Without this file executing the next cell will fail with `NoSectionError: No section: 'Access'` error message.**\n",
    "\n",
    "---\n",
    "\n",
    "Communication with OCS is done through a HubClient object. To get one, we need to authenticate with the credentials in `config.ini`. The Deschutes dataset sit in a namespace called `fermenter_vessels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "hub_client = HubClient(\n",
    "    config.get(\"Access\", \"ApiVersion\"),\n",
    "    config.get(\"Access\", \"Tenant\"),\n",
    "    config.get(\"Access\", \"Resource\"),\n",
    "    config.get(\"Credentials\", \"ClientId\"),\n",
    "    config.get(\"Credentials\", \"ClientSecret\"),\n",
    ")\n",
    "\n",
    "namespace_id = config.get(\"Configurations\", \"Namespace\")\n",
    "print (f\"namespace_id: '{namespace_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b. Download data with Data Views \n",
    "<a id='section_2b'></a>\n",
    "\n",
    "The Deschutes dataset is equipped with a set of pre-defined Data Views to easily extract data related to one fermenter vessel. Each Data View is identified by a unique identifier (ID). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find predefined Data Views \n",
    "To get the list Hub pre-defined Data Views for \"ADF\" related analysis we use the `fermenter_dataview_ids` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of Dataview IDs\n",
    "dv_ids = hub_client.fermenter_dataview_ids(namespace_id, \"adf\") \n",
    "dv_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data View Structure for Fermenter Vessels\n",
    "\n",
    "Each Data View has a set of columns which can be one of two types: \n",
    "\n",
    "* Float for numerical data\n",
    "* Category for non-numerical data like string or states \n",
    "\n",
    "For example, the \"Brand\" column contains string with beer brand names while \"Status\" is the name of a state. \n",
    "\n",
    "The 6 Data Views of this notebook have all the same columns, but data is coming from different streams (stream values can be from a sensor or a calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fermenter Vessel #31\n",
    "hub_client.dataview_definition(namespace_id, dv_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fermenter Vessel #32\n",
    "hub_client.dataview_definition(namespace_id, dv_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Get Interpolated Data from Data View\n",
    "\n",
    "To use a Data View one needs:\n",
    "\n",
    "* namespace hosting the Data View\n",
    "* its ID\n",
    "* start time (also called index since OCS supports non-time indexes)\n",
    "* end time\n",
    "* interval (of interpolation)\n",
    "\n",
    "Data Views return data into CSV (Comma Separated Values) format. The Academic Hub module provides utility functions like `dataviews_interpolated_pd` that returns a ready-to-use Panda dataframe built from those CSVs. Each row starts with a timestamp, followed by the Data View columns with (interpolated) values at that time, plus followed by the Data View ID associated with that row. \n",
    "\n",
    "**Note: it may take up to 30-45 secs for data request to complete depending on the speed of your connection and computer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OCS:\n",
    "    # store brewery data into a pandas dataframe\n",
    "    all_brands_df = hub_client.dataviews_interpolated_pd(namespace_id, dv_ids, START_INDEX, END_INDEX, INDEX_INTERVAL)\n",
    "else:\n",
    "    all_brands_df = pd.read_csv(\"ADF_ALL_BRANDS.csv\")\n",
    "    \n",
    "# preview dataframe\n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Verify Data Ordering of Results\n",
    "\n",
    "The time-sorted results of the first Data View appears first, followed by the next one and so forth. The Pandas line below shows the first 3 rows of each Data View and their index in the dataframe (left hand side integer in bold). As expected, all Data Views produced the same number of rows (43201), equals to:\n",
    "\n",
    "$$\\frac{\\text{END_INDEX} - \\text{START_INDEX}}{\\text{INDEX_INTERVAL}} + 1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brands_df[all_brands_df[\"Dataview_ID\"].shift(3) != all_brands_df[\"Dataview_ID\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Create utility functions for visualizing data\n",
    "<a id='section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3a. Function for previewing data\n",
    "<a id='section_3a'></a>\n",
    "\n",
    "The *known* bad values have been eliminated from our dataset. However, there may still exist bad values which are *unknown*.\n",
    "\n",
    "A simple way to identify possible bad values is to visualize the data by plotting them. So we create the function `plot_preview_data` to accomplish this. \n",
    "\n",
    "NOTE: We want `plot_preview_data` to be such that it can be re-used to visualize data at any point in the mutiple data cleansing steps that will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary associating dataview id with vessel id\n",
    "VESSEL_IDS = {dv_id:dv_id[-2:] for dv_id in dv_ids}\n",
    "\n",
    "def plot_preview_data(\n",
    "    df,\n",
    "    X_attribute,\n",
    "    Y_attribute,\n",
    "    all_group_ids,\n",
    "    layout,\n",
    "    mode=\"lines+markers\",\n",
    "    group_attribute=\"Dataview_ID\",\n",
    "    vessel_ids=VESSEL_IDS,\n",
    "):\n",
    "\n",
    "    # create list for plotly objects\n",
    "    plot_objects = []\n",
    "\n",
    "    # go through each fermentor vessel\n",
    "    for group_id in all_group_ids:\n",
    "\n",
    "        # create boolean for extracting data for time and vessels of interest\n",
    "        group_boolean = df[group_attribute] == group_id\n",
    "\n",
    "        # get some labels for plotting\n",
    "        if \"Dataview_ID\" in group_attribute:\n",
    "            name = f\"Fermentor {vessel_ids[group_id]}\"\n",
    "        elif \"Fermentation ID\" in group_attribute:\n",
    "            name = f\"Fermentor {group_id.strip().split()[-1].split('FV')[-1][:2]}\"\n",
    "\n",
    "        # create plotly object\n",
    "        plot_object = go.Scatter(\n",
    "            x=df.loc[group_boolean, X_attribute],\n",
    "            y=df.loc[group_boolean, Y_attribute],\n",
    "            mode=mode,\n",
    "            name=name,\n",
    "            hoverlabel={\"namelength\": -1},\n",
    "        )\n",
    "\n",
    "        # store plotly objects into list\n",
    "        plot_objects.append(plot_object)\n",
    "\n",
    "    # create and show plot\n",
    "    fig = go.Figure(data=plot_objects, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3b. Function for plotting fit and normality\n",
    "<a id='section_3b'></a>\n",
    "Create a function `model_analysis_plot` which  (1) plots the observed data and the model fit, (2) computes and plots residuals, and (3) computes and plots the normal scores for evaluating the quality of the model.\n",
    "\n",
    "`model_analysis_plot` should be re-useable, to easily plot and evaluate any model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis_plot(X, Y_observed, Y_model, model_name):\n",
    "\n",
    "    # compute residuals\n",
    "    residuals = Y_observed - Y_model\n",
    "\n",
    "    # normal probabilities\n",
    "    qq = stats.probplot(residuals)\n",
    "    normal_line = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "\n",
    "    # initialize subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{\"colspan\": 2}, None], [{}, {}]],\n",
    "        vertical_spacing=0.10,\n",
    "        horizontal_spacing=0.03,\n",
    "        shared_yaxes=True,\n",
    "    )\n",
    "\n",
    "    # create plot object for observed data\n",
    "    obj_observed = go.Scatter(\n",
    "        x=X, y=Y_observed, mode=\"markers\", marker=dict(color=\"black\"), name=\"observed\"\n",
    "    )\n",
    "\n",
    "    # create plot object for model data\n",
    "    obj_model = go.Scatter(\n",
    "        x=X, y=Y_model, mode=\"lines\", line=dict(color=\"red\"), name=f\"{model_name} model\"\n",
    "    )\n",
    "\n",
    "    # create plot object for residuals data\n",
    "    obj_residuals = go.Scatter(\n",
    "        x=Y_model,\n",
    "        y=residuals,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"grey\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # create plot normal scores of model residuals\n",
    "    obj_residual_scores = go.Scatter(\n",
    "        x=qq[0][0],\n",
    "        y=qq[0][1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"grey\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # create plot for normal scores of normally distributed data\n",
    "    obj_normal = go.Scatter(\n",
    "        x=normal_line,\n",
    "        y=qq[1][1] + qq[1][0] * normal_line,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # add plot objects to figure\n",
    "    fig.append_trace(obj_observed, row=1, col=1)\n",
    "    fig.append_trace(obj_model, row=1, col=1)\n",
    "    fig.append_trace(obj_residuals, row=2, col=1)\n",
    "    fig.append_trace(obj_residual_scores, row=2, col=2)\n",
    "    fig.append_trace(obj_normal, row=2, col=2)\n",
    "\n",
    "    # update x axis properties\n",
    "    fig.layout.xaxis1.update(title=\"Fermentation time (hour)\")\n",
    "    fig.layout.xaxis2.update(title=\"Predicted Values\")\n",
    "    fig.layout.xaxis3.update(title=\"Normal Scores\")\n",
    "\n",
    "    # update y axis properties\n",
    "    fig.layout.yaxis1.update(title=\"ADF\")\n",
    "    fig.layout.yaxis2.update(title=\"Residuals\")\n",
    "\n",
    "    # update plot layout\n",
    "    fig.layout.update(height=820)\n",
    "    fig.layout.update(width=900)\n",
    "\n",
    "    # show plot\n",
    "    fig.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Part 4. Pre-process brewery data\n",
    "<a id='section_4'></a>\n",
    "To create the ADF prediction curve, we need ADF vs time data for the `BRAND_OF_INTEREST`. The ADF profile for a brand (assuming none are bad batches) should look almost identical regardless of when it was fermented and at which vessel it was fermented.\n",
    "\n",
    "Thus in order to create a model that best predicts the ADF profile for some given brand, outliers and bad batches have to be removed from the training set. And that is the goal for this section.\n",
    "\n",
    "Some of the bad batches and bad data points are known beforehand, but some have to be identified graphically and manually removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4a. Filter out data with known bad values\n",
    "<a id='section_4a'></a>\n",
    "Some of the bad batches (`BAD_FERM_IDS`) in this time period is known beforehand. Some of the possible bad values (`BAD_INPUTS`) that can be encountered when sensors fail are also known. We want to filter out these values.\n",
    "\n",
    "Additionally, we want to filter out all unwanted values (e.g. data during non-fermentation stages will not add any useful information to the ADF profile). We also do not want to keep any attributes that will not help in creating a model for the ADF profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only data for the brand of interest\n",
    "all_brands_df = all_brands_df[all_brands_df[\"Brand\"] == BRAND_OF_INTEREST]\n",
    "\n",
    "# we are only interested in events in status=\"Fermentation\"\n",
    "# TODO: complete the filter expression to keep only data corresponding to the fermentation stages\n",
    "# =========== STUDENT BEGIN ==========\n",
    "all_brands_df = all_brands_df[@@@ Your code here @@@]\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "# remove all events with bad inputs for any of the attributes of interest\n",
    "for attribute in ATTRIBUTES_OF_INTEREST[1:]:\n",
    "    all_brands_df = all_brands_df[~all_brands_df[attribute].isin(BAD_INPUTS)]\n",
    "\n",
    "# remove all known bad fermentations\n",
    "all_brands_df = all_brands_df[~all_brands_df[\"Fermentation ID\"].isin(BAD_FERM_IDS)]\n",
    "\n",
    "# keep only values corresponding to attributes of interest\n",
    "all_brands_df = all_brands_df[ATTRIBUTES_OF_INTEREST]\n",
    "\n",
    "# preview first few lines of dataframe\n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4b. Preview raw ADF profile\n",
    "<a id='section_4b'></a>\n",
    "Preview what the data looks like for the first `N_days`. Plot ADF vs the dates. You will observe that the ADF profile for a batch is discontinous. This is because the ADF has been measured manually and therefore intermittently.\n",
    "\n",
    "We ultimately want a more continuous ADF profile for fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time period of interest for preview\n",
    "N_days = 30\n",
    "\n",
    "# time after Ndays\n",
    "end_time = (parser.parse(START_INDEX) + dt.timedelta(days=N_days)).isoformat()\n",
    "\n",
    "# data all fermentor ids (corresponds to different batches)\n",
    "all_ferm_ids = sorted(all_brands_df[\"Fermentation ID\"].unique())\n",
    "\n",
    "# define layout of plot\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Timestamp (days)\"),\n",
    "    yaxis=dict(title=\"ADF\"),\n",
    "    width=800,\n",
    "    title=f\"Preview of ADF during fermentation stage, first {N_days} days\",\n",
    ")\n",
    "\n",
    "# plot preview of data\n",
    "plot_preview_data(\n",
    "    all_brands_df[all_brands_df[\"Timestamp\"] <= end_time],\n",
    "    \"Timestamp\",\n",
    "    \"ADF\",\n",
    "    all_ferm_ids,\n",
    "    layout,\n",
    "    group_attribute=\"Fermentation ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4c. Create smoothed ADF profile\n",
    "<a id='section_4c'></a>\n",
    "In order to create a predictive model for ADF, the start of fermentation and the fermentation times should be identified. It is known that the total duration of the fermentation phase does not exceed 2.5 days, and that at the start of fermentation, the ADF should approximately equal zero.\n",
    "\n",
    "Furthermore, since ADF is measured intermittently, keep only events (rows) that correspond to new ADF measurements. This should result in a more continuous (smooth) ADF profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fermentation should not be less than zero days and should not exceed 2.5 days\n",
    "# TODO: Complete the expression\n",
    "# =========== STUDENT BEGIN ==========\n",
    "start_time = dt.timedelta(days=0).total_seconds() / 3600.0\n",
    "end_time = @@@ Your code here @@@\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "# ensure that ADF values are of the right type\n",
    "all_brands_df = all_brands_df.astype({\"ADF\": \"float\"})\n",
    "\n",
    "# keep only entries that correspond to new ADF measurements (i.e. previous ADF does not match current ADF)\n",
    "all_brands_df = all_brands_df[\n",
    "    all_brands_df[\"ADF\"].shift(1) != all_brands_df[\"ADF\"]\n",
    "].reset_index()\n",
    "\n",
    "# arrange data into chronological order\n",
    "all_brands_df.sort_values(by=\"Timestamp\", inplace=True)\n",
    "\n",
    "# find the start of fermentation (ADF ~= 0.0)\n",
    "fermentation_starts = all_brands_df[abs(all_brands_df[\"ADF\"]) <= 0.000001].reset_index()\n",
    "\n",
    "# create new dataframe column for recording the time evolution of fermentation\n",
    "all_brands_df[\"Elapsed\"] = -1\n",
    "\n",
    "# compute the time elapsed since the starts of a fermentation\n",
    "for count, ferm_begin in fermentation_starts.iterrows():\n",
    "\n",
    "    # get the timestamp for each start of fermentation\n",
    "    fermentation_time = pd.Timestamp(ferm_begin[\"Timestamp\"])\n",
    "\n",
    "    # get booleans for the new fermentation batch\n",
    "    batch_boolean = (\n",
    "        all_brands_df[\"Timestamp\"].apply(lambda t: pd.Timestamp(t)) >= fermentation_time\n",
    "    )\n",
    "\n",
    "    # find time elapsed since the start of fermentation in units of hours\n",
    "    all_brands_df.loc[batch_boolean, \"Elapsed\"] = all_brands_df.loc[\n",
    "        batch_boolean, \"Timestamp\"\n",
    "    ].apply(lambda t: (pd.Timestamp(t) - fermentation_time).total_seconds() / 3600.0)\n",
    "\n",
    "# get booleans for inconsistent data\n",
    "inconsistent_boolean = (all_brands_df[\"Elapsed\"] >= start_time) & (\n",
    "    all_brands_df[\"Elapsed\"] <= end_time\n",
    ")\n",
    "\n",
    "# remove inconsistent data\n",
    "# TODO: Complete the expression. \n",
    "# HINT: Keep all columns in ATTRIBUTES OF INTEREST + [\"Elapsed\"]\n",
    "# =========== STUDENT BEGIN =========\n",
    "all_brands_df = all_brands_df.loc[\n",
    "    @@@ Your code here @@@\n",
    "]\n",
    "# =========== STUDENT END =========\n",
    "\n",
    "# sort data from t=0 to t=tmax\n",
    "all_brands_df.sort_values(by=\"Elapsed\", inplace=True)\n",
    "\n",
    "# preview dataframe\n",
    "all_brands_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4d. Preview smooth ADF profile\n",
    "<a id='section_4d'></a>\n",
    "Preview the smoothed ADF profile. Instead of plotting it with respect to the timestamp, plot it with respect to the elapsed time of fermentation computed in Part 4c. To perform the plotting, we re-use the function `plot_preview_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dv_ids = sorted(all_brands_df[\"Dataview_ID\"].unique())\n",
    "\n",
    "# define layout of plot\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Fermentation Time (hours)\"),\n",
    "    yaxis=dict(title=\"ADF\"),\n",
    "    width=800,\n",
    "    title=f\"ADF -- {BRAND_OF_INTEREST}\",\n",
    ")\n",
    "\n",
    "# plot preview of data\n",
    "plot_preview_data(all_brands_df, \"Elapsed\", \"ADF\", all_dv_ids, layout, mode=\"markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4e. Manually remove outliers\n",
    "<a id='section_4e'></a>\n",
    "The plot in Part 4d showed some outliers. These need to be removed. Since there are not many of them, the simplest strategy is to remove them manually. Manually removing outliers is a valid data cleansing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out outliers\n",
    "# TODO: Complete filter expressions. HINT: We want data *not* in some outlier region!\n",
    "# HINT: We want data *not* in some outlier region!\n",
    "# TIP: May use as many filter expressions as necessary\n",
    "# =========== STUDENT BEGIN ==========\n",
    "all_brands_df = all_brands_df[\n",
    "    ~((all_brands_df[\"Elapsed\"] < 20) & (all_brands_df[\"ADF\"] > 0.5))\n",
    "    & @@@ Your code here @@@\n",
    "    & @@@ Your code here ??? @@@\n",
    "]\n",
    "# =========== STUDENT END ==========\n",
    "\n",
    "# define layout of plot\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title=\"Fermentation Time (hours)\"),\n",
    "    yaxis=dict(title=\"ADF\"),\n",
    "    width=800,\n",
    "    title=f\"ADF (no outliers) -- {BRAND_OF_INTEREST}\",\n",
    ")\n",
    "\n",
    "# plot preview of data\n",
    "plot_preview_data(all_brands_df, \"Elapsed\", \"ADF\", all_dv_ids, layout, mode=\"markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Part 5. Fitting and analysis\n",
    "<a id='section_5e'></a>\n",
    "Fit the data to a model and then asses the quality of that fit by looking at plots of the residuals (qualitative) and the normal scores (quantitative). We consider two models, linear and piecewise linear.\n",
    "\n",
    "For the linear model, we have model parameters $\\alpha$ and $\\beta$. The $\\epsilon$ is the difference between the observed values and the model predictions. If the model is appropriate, the $\\epsilon$ should be uncorrelated with each other.\n",
    "\n",
    "$Y = \\alpha + \\beta X + \\epsilon$\n",
    "\n",
    "The piecewise linear function we are considering has three breaks and 9 model parameters.\n",
    "\n",
    "$Y = \n",
    "    \\begin{cases}\n",
    "    \\alpha_1 + \\beta_1 X + \\epsilon, & X \\leq \\gamma_1 \\\\\n",
    "    \\alpha_2 + \\beta_2 X + \\epsilon, & \\gamma_1 \\leq X \\leq \\gamma_2 \\\\\n",
    "    \\alpha_3 + \\beta_3 X + \\epsilon, & \\gamma_2 \\leq X\n",
    "    \\end{cases}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5a. Fitting to a linear model\n",
    "<a id='section_5a'></a>\n",
    "Define the training set, apply linear regression on the training set. Compare the model fit with observations, and gauge the quality of the model fit by looking at a plot of the residuals. The residuals should not have a trend (i.e. are normally distributed). One way to test whether a process is normally distributed is through a \"normality test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before any fitting can be performed, the training set has to be defined\n",
    "X_observed = all_brands_df[\"Elapsed\"].to_frame()\n",
    "Y_observed = all_brands_df[\"ADF\"].to_frame()\n",
    "\n",
    "# create the linear model. use existing libraries\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_observed, Y_observed)\n",
    "Y_linear = regr.predict(X_observed).transpose()[0]\n",
    "\n",
    "# graphically compoar observed data vs linear model\n",
    "model_analysis_plot(X_observed[\"Elapsed\"], Y_observed[\"ADF\"], Y_linear, \"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5b. Fitting to a piecewise linear model\n",
    "<a id='section_5b'></a>\n",
    "Perform peicewise linear regression on the training set defined previously. Graphically compare the model fit with the observations; analyze the residuals and the results for the normality test.\n",
    "\n",
    "Our piecewise linear model has three breaks corresponding to the \"lag phase\", \"growth phase\", and \"fermentation phase\" in a normal brewery fermentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the piecewise linear model\n",
    "my_pwlf = pwlf.PiecewiseLinFit(X_observed[\"Elapsed\"], Y_observed[\"ADF\"])\n",
    "breaks = my_pwlf.fit(3)\n",
    "Y_pwlf = my_pwlf.predict(X_observed[\"Elapsed\"])\n",
    "\n",
    "# graphically compare observed data vs piecewise linear model\n",
    "model_analysis_plot(X_observed[\"Elapsed\"], Y_observed[\"ADF\"], Y_pwlf, \"Piecewise Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:**\n",
    "\n",
    "1. Which of the models used in this notebook performed the best and why?\n",
    "2. Can you think of a different model that would perform better than either of the models used in this notebook?\n",
    "3. Are there any trade-offs to using more complex models? If so, what are they\n",
    "4. Enumerate the reasons why we can't immediately use the raw process data for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "569px",
    "width": "680px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
